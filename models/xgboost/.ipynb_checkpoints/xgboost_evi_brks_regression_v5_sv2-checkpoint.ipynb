{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5796f876",
   "metadata": {},
   "source": [
    "## Analysis of EVI Trend Break Magnitudes with XGBRegressor - v5 sv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e34ca0",
   "metadata": {},
   "source": [
    "Peter R., 2023-08-24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f9ee6",
   "metadata": {},
   "source": [
    "### Intro\n",
    "There are several way to carry our Extreme Gradient Boosting (XGB). Here I use XGBRegression() to analyze forest EVI breaks (both negative & positive breaks) together.\n",
    "\n",
    "Here I try to answer the question: What factors predict EVI (negative and positive) Trend Break magnitude?\n",
    "\n",
    "Note that driver data have been assigned here by using spatio-temporal matches between breaks and remote sensing derived disturbance data. So far, I only have data for three drivers: fire, harvest, & insects.  The driver data are mainly nulls as I was not able to match most of the EVI breaks with disturbance data.  An important missing driver is likely tree windthrow.\n",
    "\n",
    "This is a simplified version of v5 meant to be run on DRAC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c656c99",
   "metadata": {},
   "source": [
    "#### Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "911fd651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB version: 1.7.6\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=0.4, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "Mean MAE: 517.899 (64.482)\n",
      "Mean MSE: 517.899 (64.482)\n",
      "Mean Var. Explained: 0.197 (0.098)\n",
      "R-sq: 0.185 (0.101)\n",
      "MSE: 497906.63\n",
      "RMSE: 705.62\n",
      "R-sq: 0.15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import nan\n",
    "import xgboost as xgb\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "print(\"XGB version:\", xgb.__version__)\n",
    "\n",
    "# Windows\n",
    "df1 = pd.read_csv(r'.\\data\\forest_evi_breaks_sam3.csv', skipinitialspace=True)\n",
    "# DRAC\n",
    "#df1 = pd.read_csv(r'./data/forest_evi_breaks_sam3.csv', skipinitialspace=True)\n",
    "#df1.head()\n",
    "\n",
    "df2 = pd.get_dummies(df1, columns=['protected'], dtype=float)\n",
    "\n",
    "df2= df2[df2['precipitation'].notna()]\n",
    "\n",
    "X1 = df2.iloc[:,2:24]\n",
    "# conifers & deciduous are highly negatively correlated. I drop conifers\n",
    "# Also, precipitation is correlated with CMI. I will drp cmi, cmi_lag1, cmi_lag2, cmi_lag3.\n",
    "X1.drop(X1.columns[[2, 12, 14, 16, 18]], axis=1,inplace=True)\n",
    "\n",
    "y1 = df2.iloc[:,1]\n",
    "#print(\"Response/label data\")\n",
    "#print(y1.head())\n",
    "\n",
    "\n",
    "features_names1 = [\"age\",\"deciduous\",\"elevation\",\"precipitation\",\"temperature\",\"precipitation_lag1\", \"temperature_lag1\", \"precipitation_lag2\", \"temperature_lag2\", \"precipitation_lag3\", \"temperature_lag3\",\n",
    "                 \"rh\" ,\"rh_lag1\",\"rh_lag2\",\"rh_lag3\"]\n",
    "\n",
    "\n",
    "# Fine tune parameters using GridSearchCV (for exhaustive searches) or RandomizedSearchCV (faster)\n",
    "# max_depth is tree complexity in Elith et al. 2008\n",
    "# n_estimators=100 is the number of trees. Elith et al. 2008 say this should be 1000 at least\n",
    "# Elith et al. 2008 suggests low learning rate\n",
    "\n",
    "\n",
    "seed = 7 # random seed to help with replication\n",
    "testsize1 = 0.2 # percent of records to test after training\n",
    "\n",
    "# Split data set. Note the 'stratify' option\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=testsize1, random_state=seed)\n",
    "\n",
    "\n",
    "# Fine tunning parameters with Random Search\n",
    "#search space\n",
    "params_xgboost = {\n",
    " #\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
    " \"learning_rate\"    : [ 0.01, 0.05, 0.10, 0.15, 0.20, 0.25],\n",
    " #\"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    " \"max_depth\"        : [ 3, 4, 5, 6, 8],\n",
    " #\"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    " #\"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    " \"gamma\"            : [ 0.0, 0.05, 0.1, 0.2, 0.3, 0.4],\n",
    " #\"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7],\n",
    " #'n_estimators'     : [5, 10, 15, 20, 25, 30, 35],\n",
    "'n_estimators'     : [300],\n",
    " 'objective': ['reg:squarederror'],\n",
    "#'early_stopping_rounds': [10]\n",
    "}\n",
    "\n",
    "model_m2b = XGBRegressor()\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator = model_m2b, \n",
    "                      param_distributions = params_xgboost, \n",
    "                      n_iter = 100, \n",
    "                      cv = 5, \n",
    "                      verbose=10, \n",
    "                      random_state=42,\n",
    "                      scoring = 'neg_mean_squared_error', \n",
    "                      n_jobs = -1)\n",
    "\n",
    "#params glare proba\n",
    "random_search.fit(x1_train, y1_train)\n",
    "\n",
    "print(random_search.best_estimator_)\n",
    "\n",
    "# How to early_stopping_rounds=10?\n",
    "model_m2e = random_search.best_estimator_\n",
    "\n",
    "# Best model\n",
    "# Optimal model according to random search. early_stopping_rounds conflicts with cross_val_score\n",
    "# when I run the model with all the dat then use early_stopping_rounds=10\n",
    "#model_m2e = random_search.best_estimator_\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=seed)\n",
    "\n",
    "# evaluate model\n",
    "# -1 means using all processors in parallel\n",
    "# cross val takes place withing the train data set\n",
    "scores = cross_val_score(model_m2e, x1_train, y1_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )\n",
    "\n",
    "scores2 = cross_val_score(model_m2e, x1_train, y1_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores2 = absolute(scores)\n",
    "print('Mean MSE: %.3f (%.3f)' % (scores2.mean(), scores2.std()) )\n",
    "\n",
    "# evaluate model with variance explained\n",
    "scores3 = cross_val_score(model_m2e, x1_train, y1_train, scoring='explained_variance', cv=cv, n_jobs=-1)\n",
    "#print(scores3)\n",
    "\n",
    "# force scores to be positive\n",
    "#print(statistics.mean(scores3))\n",
    "print('Mean Var. Explained: %.3f (%.3f)' % (scores3.mean(), scores3.std()) ) \n",
    "\n",
    "# R-squared\n",
    "# evaluate model with variance explained\n",
    "scores4 = cross_val_score(model_m2e, x1_train, y1_train, scoring='r2', cv=cv, n_jobs=-1)\n",
    "#print(scores3)\n",
    "\n",
    "# force scores to be positive\n",
    "#print(statistics.mean(scores3))\n",
    "print('R-sq: %.3f (%.3f)' % (scores4.mean(), scores4.std()) ) \n",
    "\n",
    "\n",
    "# EVALUATION\n",
    "#from matplotlib import pyplot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "eval_set = [(x1_train, y1_train), (x1_test, y1_test)]\n",
    "#UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
    "model_m2e.fit(x1_train, y1_train, eval_set=eval_set, verbose=False)\n",
    "# make predictions for test data\n",
    "y_pred = model_m2e.predict(x1_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "#accuracy = accuracy_score(y1_test, predictions)\n",
    "#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "# retrieve performance metrics\n",
    "results = model_m2e.evals_result()\n",
    "\n",
    "#epochs = len(results['validation_0']['error'])\n",
    "#x_axis = range(0, epochs)\n",
    "#x_axis = range(0, 100)\n",
    "\n",
    "mse = mean_squared_error(y1_test, y_pred)\n",
    "#r2 = explained_variance_score(y1_test, ypred)\n",
    "r2 = r2_score(y1_test, y_pred)\n",
    "print(\"MSE: %.2f\" % mse)\n",
    "\n",
    "print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
    "\n",
    "print(\"R-sq: %.2f\" % r2)\n",
    "\n",
    "# Save model\n",
    "# save in JSON format\n",
    "model_m2e.save_model(\"model_m2e_regression2.json\")\n",
    "# save in text format\n",
    "#model_m2.save_model(\"model_m2.txt\")\n",
    "\n",
    "# Load model\n",
    "# load saved model\n",
    "#model2 = xgb.Regressor()\n",
    "#model2.load_model(\"model_regression1.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779dd6f",
   "metadata": {},
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c055ee4",
   "metadata": {},
   "source": [
    "Mostly tutorials & blogs.\n",
    "\n",
    "https://www.youtube.com/watch?v=OQKQHNCVf5k\n",
    "\n",
    "https://www.youtube.com/watch?v=GrJP9FLV3FE&t=2167s\n",
    "\n",
    "https://datascience.stackexchange.com/questions/16342/unbalanced-multiclass-data-with-xgboost\n",
    "\n",
    "https://mljar.com/blog/xgboost-save-load-python/\n",
    "\n",
    "https://machinelearningmastery.com/xgboost-for-regression/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
    "\n",
    "https://www.datatechnotes.com/2019/06/regression-example-with-xgbregressor-in.html\n",
    "\n",
    "https://github.com/parrt/dtreeviz\n",
    "\n",
    "https://stackoverflow.com/questions/37627923/how-to-get-feature-importance-in-xgboost\n",
    "\n",
    "https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7\n",
    "\n",
    "https://scikit-learn.org/stable/modules/partial_dependence.html\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/inspection/plot_partial_dependence.html#way-partial-dependence-with-different-models\n",
    "\n",
    "https://mljar.com/blog/xgboost-early-stopping/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\n",
    "https://github.com/parrt/dtreeviz\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2b3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
