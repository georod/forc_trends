{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5796f876",
   "metadata": {},
   "source": [
    "## Analysis of EVI Trend Break Magnitudes with XGBRegressor - v5 sv3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e34ca0",
   "metadata": {},
   "source": [
    "Peter R., 2023-08-25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f9ee6",
   "metadata": {},
   "source": [
    "### Intro\n",
    "There are several way to carry our Extreme Gradient Boosting (XGB). Here I use XGBRegression() to analyze forest EVI breaks (both negative & positive breaks) together.\n",
    "\n",
    "Here I try to answer the question: What factors predict EVI (negative and positive) Trend Break magnitude? Here I run two separate models, one with negative breaks and the other with positive ones.\n",
    "\n",
    "Note that driver data have been assigned here by using spatio-temporal matches between breaks and remote sensing derived disturbance data. So far, I only have data for three drivers: fire, harvest, & insects.  The driver data are mainly nulls as I was not able to match most of the EVI breaks with disturbance data.  An important missing driver is likely tree windthrow.\n",
    "\n",
    "This is a simplified version of v5 meant to be run on DRAC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c656c99",
   "metadata": {},
   "source": [
    "#### Positive breaks XGB model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec602d",
   "metadata": {},
   "source": [
    "Here I run positive breaks only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911fd651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB version: 1.7.6\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "{'reg_lambda': 100, 'reg_alpha': 100, 'objective': 'reg:squarederror', 'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.05, 'gamma': 0.01}\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=0.01, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=0.01, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "Mean MAE: 255.336 (31.703)\n",
      "Mean MSE: 255.336 (31.703)\n",
      "Mean Var. Explained: 0.103 (0.129)\n",
      "R-sq: 0.086 (0.136)\n",
      "MSE: 109146.11\n",
      "RMSE: 330.37\n",
      "R-sq: 0.10\n",
      "Total time: 41.62\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import nan\n",
    "import xgboost as xgb\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#print(cwd)\n",
    "\n",
    "# DRAC directory\n",
    "#os.chdir(\"/home/georod/projects/def-mfortin/georod/scripts/github/forc_trends/models/xgboost\")\n",
    "\n",
    "print(\"XGB version:\", xgb.__version__)\n",
    "\n",
    "# Windows\n",
    "df1 = pd.read_csv(r'.\\data\\forest_evi_breaks_positive_sam1.csv', skipinitialspace=True)\n",
    "# DRAC\n",
    "#df1 = pd.read_csv(r'./data/forest_evi_breaks_positive_v1.csv', skipinitialspace=True)\n",
    "#df1.head()\n",
    "\n",
    "df2 = pd.get_dummies(df1, columns=['protected'], dtype=float)\n",
    "\n",
    "df2= df2[df2['precipitation'].notna()]\n",
    "\n",
    "X1 = df2.iloc[:,2:24]\n",
    "\n",
    "X1.drop(X1.columns[[2, 12, 14, 16, 18]], axis=1,inplace=True)\n",
    "\n",
    "y1 = df2.iloc[:,1]\n",
    "\n",
    "\n",
    "features_names1 = [\"age\",\"deciduous\",\"elevation\",\"precipitation\",\"temperature\",\"precipitation_lag1\", \"temperature_lag1\", \"precipitation_lag2\", \"temperature_lag2\", \"precipitation_lag3\", \"temperature_lag3\",\n",
    "                 \"rh\" ,\"rh_lag1\",\"rh_lag2\",\"rh_lag3\"]\n",
    "\n",
    "\n",
    "# Fine tune parameters using RandomizedSearchCV (faster)\n",
    "# max_depth is tree complexity in Elith et al. 2008\n",
    "# n_estimators=100 is the number of trees. Elith et al. 2008 say this should be 1000 at least\n",
    "# Elith et al. 2008 suggests low learning rate\n",
    "\n",
    "seed = 7 # random seed to help with replication\n",
    "testsize1 = 0.33 # percent of records to test after training\n",
    "\n",
    "# Split data set. Note the 'stratify' option\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=testsize1, random_state=seed)\n",
    "\n",
    "\n",
    "# Fine tunning parameters with Random Search\n",
    "#search space\n",
    "params_xgboost = {\n",
    " \"learning_rate\"    : [ 0.001, 0.005, 0.01, 0.05, 0.10, 0.15],\n",
    " \"max_depth\"        : [ 3, 4, 5, 6, 8, 10],\n",
    " \"gamma\"            : [ 0.0, 0.01, 0.05, 0.1, 0.2, 0.3],\n",
    " #\"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7],\n",
    " #'n_estimators'     : [5, 10, 15, 20, 25, 30, 35],\n",
    "'n_estimators'     : [300],\n",
    " 'objective': ['reg:squarederror'],\n",
    "#'early_stopping_rounds': [10]\n",
    "# reg_alpha provides L1 regularization to the weight, higher values result in more conservative models\n",
    "\"reg_alpha\": [1e-5, 1e-2, 0.1, 1, 10, 100],\n",
    "# reg_lambda provides L2 regularization to the weight, higher values result in more conservative models\n",
    "\"reg_lambda\": [1e-5, 1e-2, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "model_base1 = XGBRegressor()\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator = model_base1, \n",
    "                      param_distributions = params_xgboost, \n",
    "                      n_iter = 100, \n",
    "                      cv = 5, \n",
    "                      verbose=10, \n",
    "                      random_state=42,\n",
    "                      scoring = 'neg_mean_squared_error', \n",
    "                        refit=True,\n",
    "                      n_jobs = -1)\n",
    "\n",
    "#params glare proba\n",
    "random_search.fit(x1_train, y1_train)\n",
    "\n",
    "#random_search\n",
    "print(random_search.best_params_)\n",
    "print(random_search.best_estimator_)\n",
    "\n",
    "# How to early_stopping_rounds=10?\n",
    "# Model with best parameters 1\n",
    "model_bp1 = random_search.best_estimator_\n",
    "\n",
    "#print(model_bp1)\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=seed)\n",
    "\n",
    "# evaluate model with train\n",
    "# -1 means using all processors in parallel\n",
    "# cross val takes place withing the train data set\n",
    "scores = cross_val_score(model_bp1, x1_train, y1_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )\n",
    "\n",
    "scores2 = cross_val_score(model_bp1, x1_train, y1_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores2 = absolute(scores)\n",
    "print('Mean MSE: %.3f (%.3f)' % (scores2.mean(), scores2.std()) )\n",
    "\n",
    "# evaluate model with variance explained\n",
    "scores3 = cross_val_score(model_bp1, x1_train, y1_train, scoring='explained_variance', cv=cv, n_jobs=-1)\n",
    "#print(scores3)\n",
    "\n",
    "# force scores to be positive\n",
    "#print(statistics.mean(scores3))\n",
    "print('Mean Var. Explained: %.3f (%.3f)' % (scores3.mean(), scores3.std()) ) \n",
    "\n",
    "# R-squared\n",
    "# evaluate model with variance explained\n",
    "scores4 = cross_val_score(model_bp1, x1_train, y1_train, scoring='r2', cv=cv, n_jobs=-1)\n",
    "#print(scores3)\n",
    "\n",
    "# force scores to be positive\n",
    "#print(statistics.mean(scores3))\n",
    "print('R-sq: %.3f (%.3f)' % (scores4.mean(), scores4.std()) ) \n",
    "\n",
    "\n",
    "# EVALUATION (with test)\n",
    "eval_set = [(x1_train, y1_train), (x1_test, y1_test)]\n",
    "#UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
    "model_bp1.fit(x1_train, y1_train, eval_set=eval_set, verbose=False)\n",
    "# make predictions for test data\n",
    "y_pred = model_bp1.predict(x1_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# retrieve performance metrics\n",
    "results = model_bp1.evals_result()\n",
    "\n",
    "mse = mean_squared_error(y1_test, y_pred)\n",
    "#r2 = explained_variance_score(y1_test, ypred)\n",
    "r2 = r2_score(y1_test, y_pred)\n",
    "print(\"MSE: %.2f\" % mse)\n",
    "\n",
    "print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
    "\n",
    "print(\"R-sq: %.2f\" % r2)\n",
    "\n",
    "# Save model\n",
    "# save in JSON format\n",
    "model_bp1.save_model(\"model_bp1_pos_brks_sam1.json\")\n",
    "# save in text format\n",
    "#model_m2.save_model(\"model_m2.txt\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "total_time = end-start\n",
    "#total_time\n",
    "print(\"Total time: %.2f\" % total_time)\n",
    "\n",
    "# Load model\n",
    "# load saved model\n",
    "#model2 = xgb.Regressor()\n",
    "#model2.load_model(\"model_regression1.json\")\n",
    "\n",
    "\n",
    "# all brks\n",
    "#Mean MAE: 517.899 (64.482)\n",
    "#Mean MSE: 517.899 (64.482)\n",
    "#Mean Var. Explained: 0.197 (0.098)\n",
    "#R-sq: 0.185 (0.101)\n",
    "#MSE: 497906.63\n",
    "#RMSE: 705.62\n",
    "#R-sq: 0.15\n",
    "\n",
    "# Take 1, positive breaks sample 1\n",
    "#Mean MAE: 255.336 (31.703)\n",
    "#Mean MSE: 255.336 (31.703)\n",
    "#Mean Var. Explained: 0.103 (0.129)\n",
    "#R-sq: 0.086 (0.136)\n",
    "#MSE: 109146.11\n",
    "#RMSE: 330.37\n",
    "#R-sq: 0.10\n",
    "#Total time: 43.21\n",
    "\n",
    "# Take 2\n",
    "#Mean MAE: 223.763 (14.087)\n",
    "#Mean MSE: 223.763 (14.087)\n",
    "#Mean Var. Explained: 0.291 (0.077)\n",
    "#R-sq: 0.287 (0.078)\n",
    "#MSE: 102980.89\n",
    "#RMSE: 320.91\n",
    "#R-sq: 0.35\n",
    "\n",
    "#Mean MAE: 255.336 (31.703)\n",
    "#Mean MSE: 255.336 (31.703)\n",
    "#Mean Var. Explained: 0.103 (0.129)\n",
    "#R-sq: 0.086 (0.136)\n",
    "#MSE: 109146.11\n",
    "#RMSE: 330.37\n",
    "#R-sq: 0.10\n",
    "#Total time: 42.47\n",
    "\n",
    "#Take 3, all positive breaks with alpha & lamda\n",
    "#Mean MAE: 223.102 (13.814)\n",
    "#Mean MSE: 223.102 (13.814)\n",
    "#Mean Var. Explained: 0.303 (0.065)\n",
    "#R-sq: 0.299 (0.065)\n",
    "#MSE: 106925.62\n",
    "#RMSE: 326.99\n",
    "#R-sq: 0.32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c7304",
   "metadata": {},
   "source": [
    "#### Negative breaks XGB model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cc3d1",
   "metadata": {},
   "source": [
    "Here I run a model with negative breaks only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8380ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB version: 1.7.6\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "{'reg_lambda': 100, 'reg_alpha': 10, 'objective': 'reg:squarederror', 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'gamma': 0.0}\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=0.0, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=5, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "Mean MAE: 238.385 (23.135)\n",
      "Mean MSE: 238.385 (23.135)\n",
      "Mean Var. Explained: 0.083 (0.115)\n",
      "R-sq: 0.074 (0.114)\n",
      "MSE: 107278.62\n",
      "RMSE: 327.53\n",
      "R-sq: 0.17\n",
      "Total time: 18.04\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import nan\n",
    "import xgboost as xgb\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#print(cwd)\n",
    "\n",
    "# DRAC directory\n",
    "#os.chdir(\"/home/georod/projects/def-mfortin/georod/scripts/github/forc_trends/models/xgboost\")\n",
    "\n",
    "print(\"XGB version:\", xgb.__version__)\n",
    "\n",
    "# Windows\n",
    "df1 = pd.read_csv(r'.\\data\\forest_evi_breaks_negative_sam1.csv', skipinitialspace=True)\n",
    "# DRAC\n",
    "#df1 = pd.read_csv(r'./data/forest_evi_breaks_negative_sam1.csv', skipinitialspace=True)\n",
    "#df1.head()\n",
    "\n",
    "df2 = pd.get_dummies(df1, columns=['protected'], dtype=float)\n",
    "\n",
    "df2= df2[df2['precipitation'].notna()]\n",
    "\n",
    "X1 = df2.iloc[:,2:24]\n",
    "\n",
    "X1.drop(X1.columns[[2, 12, 14, 16, 18]], axis=1,inplace=True)\n",
    "\n",
    "y1 = df2.iloc[:,1]\n",
    "\n",
    "\n",
    "features_names1 = [\"age\",\"deciduous\",\"elevation\",\"precipitation\",\"temperature\",\"precipitation_lag1\", \"temperature_lag1\", \"precipitation_lag2\", \"temperature_lag2\", \"precipitation_lag3\", \"temperature_lag3\",\n",
    "                 \"rh\" ,\"rh_lag1\",\"rh_lag2\",\"rh_lag3\"]\n",
    "\n",
    "\n",
    "# Fine tune parameters using RandomizedSearchCV (faster)\n",
    "# max_depth is tree complexity in Elith et al. 2008\n",
    "# n_estimators=100 is the number of trees. Elith et al. 2008 say this should be 1000 at least\n",
    "# Elith et al. 2008 suggests low learning rate\n",
    "\n",
    "seed = 7 # random seed to help with replication\n",
    "testsize1 = 0.33 # percent of records to test after training\n",
    "\n",
    "# Split data set. Note the 'stratify' option\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=testsize1, random_state=seed)\n",
    "\n",
    "\n",
    "# Fine tunning parameters with Random Search\n",
    "#search space\n",
    "params_xgboost = {\n",
    " \"learning_rate\"    : [ 0.001, 0.005, 0.01, 0.05, 0.10, 0.15],\n",
    " \"max_depth\"        : [ 3, 4, 5, 6, 8, 10],\n",
    " \"gamma\"            : [ 0.0, 0.01, 0.05, 0.1, 0.2, 0.3],\n",
    " #\"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7],\n",
    " #'n_estimators'     : [5, 10, 15, 20, 25, 30, 35],\n",
    "'n_estimators'     : [100],\n",
    " 'objective': ['reg:squarederror'],\n",
    "#'early_stopping_rounds': [10]\n",
    "# reg_alpha provides L1 regularization to the weight, higher values result in more conservative models\n",
    "\"reg_alpha\": [1e-5, 1e-2, 0.1, 1, 10, 100],\n",
    "# reg_lambda provides L2 regularization to the weight, higher values result in more conservative models\n",
    "\"reg_lambda\": [1e-5, 1e-2, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "model_base1 = XGBRegressor()\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator = model_base1, \n",
    "                      param_distributions = params_xgboost, \n",
    "                      n_iter = 100, \n",
    "                      cv = 5, \n",
    "                      verbose=10, \n",
    "                      random_state=42,\n",
    "                      scoring = 'neg_mean_squared_error', \n",
    "                        refit=True,\n",
    "                      n_jobs = -1)\n",
    "\n",
    "#params glare proba\n",
    "random_search.fit(x1_train, y1_train)\n",
    "\n",
    "#random_search\n",
    "print(random_search.best_params_)\n",
    "print(random_search.best_estimator_)\n",
    "\n",
    "# How to early_stopping_rounds=10?\n",
    "# Model with best parameters 1\n",
    "model_bp1 = random_search.best_estimator_\n",
    "\n",
    "#print(model_bp1)\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=seed)\n",
    "\n",
    "# evaluate model with train\n",
    "# -1 means using all processors in parallel\n",
    "# cross val takes place withing the train data set\n",
    "scores = cross_val_score(model_bp1, x1_train, y1_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )\n",
    "\n",
    "scores2 = cross_val_score(model_bp1, x1_train, y1_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores2 = absolute(scores)\n",
    "print('Mean MSE: %.3f (%.3f)' % (scores2.mean(), scores2.std()) )\n",
    "\n",
    "# evaluate model with variance explained\n",
    "scores3 = cross_val_score(model_bp1, x1_train, y1_train, scoring='explained_variance', cv=cv, n_jobs=-1)\n",
    "#print(scores3)\n",
    "\n",
    "# force scores to be positive\n",
    "#print(statistics.mean(scores3))\n",
    "print('Mean Var. Explained: %.3f (%.3f)' % (scores3.mean(), scores3.std()) ) \n",
    "\n",
    "# R-squared\n",
    "# evaluate model with variance explained\n",
    "scores4 = cross_val_score(model_bp1, x1_train, y1_train, scoring='r2', cv=cv, n_jobs=-1)\n",
    "#print(scores3)\n",
    "\n",
    "# force scores to be positive\n",
    "#print(statistics.mean(scores3))\n",
    "print('R-sq: %.3f (%.3f)' % (scores4.mean(), scores4.std()) ) \n",
    "\n",
    "\n",
    "# EVALUATION (with test)\n",
    "eval_set = [(x1_train, y1_train), (x1_test, y1_test)]\n",
    "#UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
    "model_bp1.fit(x1_train, y1_train, eval_set=eval_set, verbose=False)\n",
    "# make predictions for test data\n",
    "y_pred = model_bp1.predict(x1_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# retrieve performance metrics\n",
    "results = model_bp1.evals_result()\n",
    "\n",
    "mse = mean_squared_error(y1_test, y_pred)\n",
    "#r2 = explained_variance_score(y1_test, ypred)\n",
    "r2 = r2_score(y1_test, y_pred)\n",
    "print(\"MSE: %.2f\" % mse)\n",
    "\n",
    "print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
    "\n",
    "print(\"R-sq: %.2f\" % r2)\n",
    "\n",
    "# Save model\n",
    "# save in JSON format\n",
    "model_bp1.save_model(\"model_bp1_neg_brks_sam1.json\")\n",
    "# save in text format\n",
    "#model_m2.save_model(\"model_m2.txt\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "total_time = end-start\n",
    "#total_time\n",
    "print(\"Total time: %.2f\" % total_time)\n",
    "\n",
    "# Load model\n",
    "# load saved model\n",
    "#model2 = xgb.Regressor()\n",
    "#model2.load_model(\"model_regression1.json\")\n",
    "\n",
    "# Take 1, neg brks\n",
    "#Mean MAE: 236.620 (25.180)\n",
    "#Mean MSE: 236.620 (25.180)\n",
    "#Mean Var. Explained: 0.084 (0.067)\n",
    "#R-sq: 0.060 (0.072)\n",
    "#MSE: 111362.78\n",
    "#RMSE: 333.71\n",
    "#R-sq: 0.14\n",
    "\n",
    "# Take 2\n",
    "#Mean MAE: 238.385 (23.135)\n",
    "#Mean MSE: 238.385 (23.135)\n",
    "#Mean Var. Explained: 0.083 (0.115)\n",
    "#R-sq: 0.074 (0.114)\n",
    "#MSE: 107278.62\n",
    "#RMSE: 327.53\n",
    "#R-sq: 0.17\n",
    "#Total time: 18.04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779dd6f",
   "metadata": {},
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c055ee4",
   "metadata": {},
   "source": [
    "Mostly tutorials & blogs.\n",
    "\n",
    "https://www.youtube.com/watch?v=OQKQHNCVf5k\n",
    "\n",
    "https://www.youtube.com/watch?v=GrJP9FLV3FE&t=2167s\n",
    "\n",
    "https://datascience.stackexchange.com/questions/16342/unbalanced-multiclass-data-with-xgboost\n",
    "\n",
    "https://mljar.com/blog/xgboost-save-load-python/\n",
    "\n",
    "https://machinelearningmastery.com/xgboost-for-regression/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
    "\n",
    "https://www.datatechnotes.com/2019/06/regression-example-with-xgbregressor-in.html\n",
    "\n",
    "https://github.com/parrt/dtreeviz\n",
    "\n",
    "https://stackoverflow.com/questions/37627923/how-to-get-feature-importance-in-xgboost\n",
    "\n",
    "https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7\n",
    "\n",
    "https://scikit-learn.org/stable/modules/partial_dependence.html\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/inspection/plot_partial_dependence.html#way-partial-dependence-with-different-models\n",
    "\n",
    "https://mljar.com/blog/xgboost-early-stopping/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\n",
    "https://github.com/parrt/dtreeviz\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2b3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
