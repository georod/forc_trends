{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70fe0354",
   "metadata": {},
   "source": [
    "# Positive breaks XGB model - local run\n",
    "\n",
    "Given that DRAC was down on Feb. 22 and 23, 2024, I had to run DRAC XGBoost models locally. (Once, DRAC is up again I can run these models there.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da472a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-02-20\n",
    "# Peter R.\n",
    "# XGBoost script\n",
    "# Positive breaks, n_estimators (number of trees)=1000 and with optimal parameter from DRAC model_bp1 & early stopping\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import nan\n",
    "import xgboost as xgb\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#print(cwd)\n",
    "\n",
    "# DRAC directory\n",
    "#os.chdir(\"/home/georod/projects/def-mfortin/georod/scripts/github/forc_trends/models/xgboost\")\n",
    "# Win directory\n",
    "os.chdir(r'C:\\Users\\Peter R\\github\\forc_trends\\models\\xgboost')\n",
    "\n",
    "\n",
    "print(\"XGB version:\", xgb.__version__)\n",
    "print(\"positive breaks\")\n",
    "\n",
    "# Windows\n",
    "df1 = pd.read_csv(r'.\\data\\forest_evi_breaks_positive_v2.csv', skipinitialspace=True)\n",
    "# DRAC\n",
    "#df1 = pd.read_csv(r'./data/forest_evi_breaks_positive_v2.csv', skipinitialspace=True)\n",
    "#df1.head()\n",
    "\n",
    "df2 = pd.get_dummies(df1, columns=['for_pro'], dtype=float)\n",
    "\n",
    "#df2= df2[df2['precipitation'].notna()]\n",
    "\n",
    "#X1 = df2.iloc[:,2:30]\n",
    "\n",
    "#X1.drop(X1.columns[[0:5, 7:10, 12, 15, 16, 18, 19, 20,21,22,23,24,25]], axis=1,inplace=True)\n",
    "\n",
    "#cols2 = ['for_age','for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt', 'nffd_wt', 'nffd_wt_lag1', 'nffd_wt_lag2', 'nffd_wt_lag3', 'pas_sm', 'pas_sm_lag1', 'pas_sm_lag2', 'pas_sm_lag3', 'for_pro_0']\n",
    "#cols2 = ['for_age','for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt', 'nffd_wt', 'nffd_wt_lag1', 'nffd_wt_lag2', 'nffd_wt_lag3', 'pas_sm', 'pas_sm_lag1', 'pas_sm_lag2', 'pas_sm_lag3', 'for_pro_0', 'map', 'map_lag1', 'map_lag2', 'map_lag3','mat', 'mat_lag1', 'mat_lag2', 'mat_lag3','rh', 'rh_lag1', 'rh_lag2', 'rh_lag3']\n",
    "cols2 =  ['for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt', 'dd5_wt_lag2']\n",
    "#cols2 = ['for_age','for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt', 'dd5_wt_lag2', 'for_pro_0', 'for_pro_1']\n",
    "X1 = df2[cols2]\n",
    "\n",
    "# vars used in previous version: 'map', 'map_lag1', 'map_lag2', 'map_lag3','mat', 'mat_lag1', 'mat_lag2', 'mat_lag3','rh', 'rh_lag1', 'rh_lag2', 'rh_lag3'\n",
    "#features_names1 = [\"age\",\"deciduous\",\"elevation\",\"precipitation\",\"temperature\",\"precipitation_lag1\", \"temperature_lag1\", \"precipitation_lag2\", \"temperature_lag2\", \"precipitation_lag3\", \"temperature_lag3\",\n",
    "#                 \"rh\" ,\"rh_lag1\",\"rh_lag2\",\"rh_lag3\"]\n",
    "\n",
    "y1 = df2.iloc[:,6]\n",
    "\n",
    "# Fine tune parameters using RandomizedSearchCV (faster)\n",
    "# max_depth is tree complexity in Elith et al. 2008\n",
    "# n_estimators=100 is the number of trees. Elith et al. 2008 say this should be 1000 at least\n",
    "# Elith et al. 2008 suggests low learning rate\n",
    "\n",
    "seed = 7 # random seed to help with replication\n",
    "testsize1 = 0.33 # percent of records to test after training\n",
    "\n",
    "# Split data set. Note the 'stratify' option\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=testsize1, random_state=seed)\n",
    "\n",
    "\n",
    "\n",
    "#{'reg_lambda': 10, 'reg_alpha': 0.1, 'objective': 'reg:squarederror', 'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.005, 'gamma': 0.05}\n",
    "\n",
    "#{'reg_lambda': 10, 'reg_alpha': 1, 'objective': 'reg:squarederror', 'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.01, 'gamma': 0.2}\n",
    "            \n",
    "model_bp2 = XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
    "             colsample_bylevel=None, colsample_bynode=None,\n",
    "             colsample_bytree=None, early_stopping_rounds=50,\n",
    "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "             gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
    "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
    "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "             n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
    "             predictor=None, random_state=42, reg_lambda=10, reg_alpha=1)\n",
    "\n",
    "\n",
    "\n",
    "# EVALUATION (with test)\n",
    "eval_set = [(x1_train, y1_train), (x1_test, y1_test)]\n",
    "#UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
    "model_bp2.fit(x1_train, y1_train, eval_set=eval_set, verbose=False)\n",
    "# make predictions for test data\n",
    "y_pred = model_bp2.predict(x1_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# retrieve performance metrics\n",
    "results = model_bp2.evals_result()\n",
    "\n",
    "mse = mean_squared_error(y1_test, y_pred)\n",
    "#r2 = explained_variance_score(y1_test, ypred)\n",
    "r2 = r2_score(y1_test, y_pred)\n",
    "print(\"MSE: %.2f\" % mse)\n",
    "\n",
    "print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
    "\n",
    "print(\"R-sq: %.2f\" % r2)\n",
    "\n",
    "# Save model\n",
    "# save in JSON format\n",
    "#model_bp1.save_model(\"model_bp1_pos_brks_v1.json\")\n",
    "#model_bp2.save_model(\"model_bp2_pos_brks_v3.json\") # Take 2 with new climate vars\n",
    "#model_bp2.save_model(\"model_bp2_pos_brks_v4.json\") # Take 2 with new climate vars + old vars\n",
    "model_bp2.save_model(\"model_bp2_pos_brks_v5_local.json\") # Take 3 with VIF var subset\n",
    "#model_bp2.save_model(\"model_bp2_pos_brks_v6.json\") # Take 3 with VIF var subset + a few old vars (for_age, etc.)\n",
    "# save in text format\n",
    "#model_m2.save_model(\"model_m2.txt\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "total_time = end-start\n",
    "#total_time\n",
    "print(\"Total time: %.2f\" % total_time)\n",
    "\n",
    "# Load model\n",
    "# load saved model\n",
    "#model2 = xgb.Regressor()\n",
    "#model2.load_model(\"model_regression1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb939793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
