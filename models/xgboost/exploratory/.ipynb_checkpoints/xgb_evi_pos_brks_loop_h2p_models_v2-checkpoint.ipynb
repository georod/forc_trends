{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9dab20",
   "metadata": {},
   "source": [
    "# Positive breaks XGBoost model - local run with loop\n",
    "\n",
    "### Peter R.\n",
    "#### 2024-03-12\n",
    "\n",
    "NOte: Here I use h2p, that is Bfast breaks with h=0.0025 or ~ 2%. The filtering of records with h2p is different than with h5p.\n",
    "\n",
    "Today (2024-03-04) I meet with MJF to discuss XGB model improvements. An important number of Bfast breaks have very wide confidence intervals (CIs) associated with the time of break. These CIs can range from about 1 month to 80+ months. These won't allow for high quality matching with yearly climate or disturbance data. For this reason, we decided to run XGB models with subsets of data, each subset has narrow CIs. We will run the following XGB models with the follwoing dataframe subsets:\n",
    "\n",
    "- Dataframe3 (df3): Records with CIs shorter than 3 16-days data points (48 days or about 1.5 months)\n",
    "\n",
    "- Dataframe6 (df6): Records with CIs shorter than 6 16-days data points (96 days or about 3 months)\n",
    "\n",
    "- Dataframe9 (df9): Records with CIs shorter than 9 16-days data points (144 days or about 5 months)\n",
    "\n",
    "- Dataframe23 (df23): Records with CIS shorter than 23 16-days data points (368 days or about 1 year)\n",
    "\n",
    "Some questions to have in mind:\n",
    "\n",
    "- How many matches with disturbance data do the above have?\n",
    "- Why does forest age become the top ranking variable with VIFplust variable set?  This variable was number 10 in other previous XGB model.\n",
    "- I am assuming that Hansen is best and that it only includes stand-replacing disturbances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82ed8877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB version: 1.7.6\n",
      "negative breaks\n"
     ]
    }
   ],
   "source": [
    "# 2024-03-04\n",
    "# Peter R.\n",
    "# XGBoost script\n",
    "# Positive breaks, n_estimators (number of trees)=1000 and with optimal parameter from DRAC model_bp1 & early stopping\n",
    "\n",
    "#Here I am using a loop to run several models at a time\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import nan\n",
    "import xgboost as xgb\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# for feature importance plots\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#for dependency plots\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "#start = time.time()\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#print(cwd)\n",
    "\n",
    "# DRAC directory\n",
    "#os.chdir(\"/home/georod/projects/def-mfortin/georod/scripts/github/forc_trends/models/xgboost\")\n",
    "# Win directory\n",
    "os.chdir(r'C:\\Users\\Peter R\\github\\forc_trends\\models\\xgboost')\n",
    "\n",
    "\n",
    "print(\"XGB version:\", xgb.__version__)\n",
    "print(\"negative breaks\")\n",
    "\n",
    "\n",
    "# Windows\n",
    "df1 = pd.read_csv(r'.\\data\\forest_evi_breaks_positive_h2p_v3.csv', skipinitialspace=True)\n",
    "# DRAC\n",
    "#df1 = pd.read_csv(r'./data/forest_evi_breaks_positive_v2.csv', skipinitialspace=True)\n",
    "#df1.head()\n",
    "\n",
    "df11 = pd.get_dummies(df1, columns=['for_pro'], dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1994c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Df0: all records\n",
    "df0 = df11 # N=687\n",
    "# Df3: 1.5 months\n",
    "df3 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.1315068) & (df11['magnitude']> 700)] #N=40\n",
    "# Df6: 3 months\n",
    "df6 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.2630137) & (df11['magnitude']> 700)] #N=168\n",
    "# Df9: 5 months, version 4\n",
    "df9 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.3945205) & (df11['magnitude']> 700)] #N=649\n",
    "# Df23: 12 months, 1 year, version 5\n",
    "df23 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 1.008219) & (df11['magnitude']> 700)] #N=2216\n",
    "\n",
    "#dfall = [[df0],[df3], [df6], [df9], [df23]]\n",
    "dfall = [df0, df3, df6, df9, df23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9078a73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(df2[['canlad_year']].describe())#\n",
    "#print(df2[['hansen_year']].describe())#\n",
    "#print(df2.describe()) #Full data: 11969; df4=1360; df5=5861\n",
    "#(dfall[0])\n",
    "#print(dfall[4].describe())\n",
    "#print((dfall[0]).head())\n",
    "range(len(dfall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57147b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | df0 | 15 | 950867.16 | 975.12 | -0.183 | -0.267 | 687 | 15 |\n",
      "1 | df0 | 7 | 866674.11 | 930.95 | -0.078 | -0.112 | 687 | 7 |\n",
      "1 | df0 | 9 | 870364.27 | 932.93 | -0.082 | -0.127 | 687 | 9 |\n",
      "2 | df3 | 15 | 794865.30 | 891.55 | -0.104 | -0.463 | 186 | 15 |\n",
      "2 | df3 | 7 | 808993.20 | 899.44 | -0.123 | -0.269 | 186 | 7 |\n",
      "2 | df3 | 9 | 830910.29 | 911.54 | -0.154 | -0.353 | 186 | 9 |\n",
      "3 | df6 | 15 | 641315.04 | 800.82 | -0.084 | -0.224 | 399 | 15 |\n",
      "3 | df6 | 7 | 614096.13 | 783.64 | -0.038 | -0.097 | 399 | 7 |\n",
      "3 | df6 | 9 | 621438.85 | 788.31 | -0.050 | -0.128 | 399 | 9 |\n",
      "4 | df9 | 15 | 707250.58 | 840.98 | -0.212 | -0.347 | 456 | 15 |\n",
      "4 | df9 | 7 | 653417.75 | 808.34 | -0.120 | -0.175 | 456 | 7 |\n",
      "4 | df9 | 9 | 698990.70 | 836.06 | -0.198 | -0.274 | 456 | 9 |\n",
      "5 | df23 | 15 | 555730.01 | 745.47 | 0.055 | -0.036 | 517 | 15 |\n",
      "5 | df23 | 7 | 502591.44 | 708.94 | 0.146 | 0.109 | 517 | 7 |\n",
      "5 | df23 | 9 | 514135.96 | 717.03 | 0.126 | 0.077 | 517 | 9 |\n"
     ]
    }
   ],
   "source": [
    "# loop version\n",
    "\n",
    "cols1 = ['for_age', 'for_con', 'map', 'map_lag1', 'map_lag2', 'map_lag3', 'mat', 'mat_lag1', 'mat_lag2', 'mat_lag3', 'rh', 'rh_lag1', 'rh_lag2', 'rh_lag3', 'for_pro_0']\n",
    "cols2 = ['for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt_lag1', 'dd5_wt_lag3']\n",
    "cols3 = ['for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt_lag1', 'dd5_wt_lag3', 'for_age', 'for_pro_0']\n",
    "\n",
    "df_labs = ['df0','df3', 'df6', 'df9', 'df23']\n",
    "\n",
    "for z in range(len(dfall)):\n",
    "    list_of_vars = [[cols1], [cols2], [cols3]]\n",
    "    model_labs = ['First variable set', 'VIF variable set', 'VIFplus variable set']\n",
    "    for list in list_of_vars:\n",
    "        for x in list:\n",
    "            #print(x)\n",
    "            X1 = dfall[z][x]\n",
    "            #print(X1.describe())\n",
    "            y1 = dfall[z].iloc[:,6].abs()\n",
    "            seed = 7 # random seed to help with replication\n",
    "            testsize1 = 0.33 # percent of records to test after training\n",
    "            x1_train, x1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=testsize1, random_state=seed) # Split data set. Note the 'stratify' option\n",
    "            model_bp2 = XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
    "                 colsample_bylevel=None, colsample_bynode=None,\n",
    "                 colsample_bytree=None, early_stopping_rounds=50,\n",
    "                 enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "                 gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "                 interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
    "                 max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "                 max_delta_step=None, max_depth=8, max_leaves=None,\n",
    "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "                 n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
    "                 predictor=None, random_state=42, reg_lambda=10, reg_alpha=1)\n",
    "               # EVALUATION (with test)\n",
    "            eval_set = [(x1_train, y1_train), (x1_test, y1_test)]\n",
    "                #UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
    "            model_bp2.fit(x1_train, y1_train, eval_set=eval_set, verbose=False)\n",
    "                # make predictions for test data\n",
    "            y_pred = model_bp2.predict(x1_test)\n",
    "            predictions = [round(value) for value in y_pred]\n",
    "                # retrieve performance metrics\n",
    "            results = model_bp2.evals_result()\n",
    "            mse = mean_squared_error(y1_test, y_pred)\n",
    "                #r2 = explained_variance_score(y1_test, ypred)\n",
    "            r2 = r2_score(y1_test, y_pred)\n",
    "                # adjusted R-squared\n",
    "            adj_r2 = 1 - (((1-r2) * (len(y1_test)-1))/(len(y1_test)-x1_test.shape[1]-1))\n",
    "\n",
    "            #print(\"MSE: %.2f\" % mse)\n",
    "            var1 = \"%.2f\" % mse\n",
    "\n",
    "            #print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
    "            var2 = \"%.2f\" % (mse**(1/2.0))\n",
    "\n",
    "            #print(\"R-sq: %.3f\" % r2)\n",
    "            var3 = \"%.3f\" % r2\n",
    "\n",
    "            #print(\"R-sq-adj: %.3f\" % adj_r2)\n",
    "            var4 = \"%.3f\" % adj_r2\n",
    "\n",
    "            var5 = X1.shape[0]\n",
    "            var6 = X1.shape[1]\n",
    "\n",
    "            # row for table\n",
    "            print(z+1, \"|\", df_labs[z], \"|\", len(x), \"| %.2f\" % mse, \"| %.2f\" % (mse**(1/2.0)), \"| %.3f\" % r2, \"| %.3f\" % adj_r2, \"|\", X1.shape[0], \"|\", X1.shape[1],\"|\")\n",
    "\n",
    "            # Feature importance plot\n",
    "            # xgb.plot_importance(model_bp2, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance, gain', \n",
    "            #            xlabel='F score - Gain', ylabel='Features', \n",
    "            #            importance_type='gain', max_num_features=15, grid=True, show_values=False) #, values_format='{v:.2f}' )\n",
    "\n",
    "            #pyplot.savefig(r'.\\figs\\version4\\h2p\\df23\\neg_gain_m{y}_v1.png'.format(y=len(x)),  dpi=300, bbox_inches='tight')\n",
    "            #pyplot.show()\n",
    "            # create lis of feature names to be used in dependency plot so that high ranking vars are plotted\n",
    "            #features_names1 = pd.DataFrame()\n",
    "            #features_names1['columns'] = X1.columns\n",
    "            #features_names1['importances'] = model_bp2.feature_importances_\n",
    "            #features_names1.sort_values(by='importances',ascending=False,inplace=True)\n",
    "            #features_names2 = features_names1['columns'].tolist()[0:10]\n",
    "\n",
    "            #_, ax1 = plt.subplots(figsize=(9, 8), constrained_layout=True)\n",
    "\n",
    "            #display = PartialDependenceDisplay.from_estimator(model_bp2, x1_train, features_names2, ax=ax1)\n",
    "\n",
    "            #_ = display.figure_.suptitle((\"Partial dependence plots\"), fontsize=12, )\n",
    "\n",
    "            #pyplot.savefig(r'.\\figs\\version4\\h2p\\df23\\neg_partial_dep_m{y}_v1.png'.format(y=len(x)),  dpi=300, bbox_inches='tight')\n",
    "\n",
    "           # pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1ddf3",
   "metadata": {},
   "source": [
    "**Table 1**: Model comparison for negative breaks. Standard data set with all records (including NAs for for_age and for_con).\n",
    "\n",
    "\n",
    "|ID|Data frame| Model   | MSE| RMSE| R-sq | R-sq-adj |N rows| N vars|\n",
    "| --------| --------| --------| --------| -------- | ------- |-------- | ------- |------- |\n",
    "|1 | df0 | First variable set | 950867.16 | 975.12 | -0.183 | -0.267 | 687 | 15 |\n",
    "|1 | df0 | VIF variable set | 866674.11 | 930.95 | -0.078 | -0.112 | 687 | 7 |\n",
    "|1 | df0 | VIFplus variable set  | 870364.27 | 932.93 | -0.082 | -0.127 | 687 | 9 |\n",
    "|2 | df3 | First variable set | 794865.30 | 891.55 | -0.104 | -0.463 | 186 | 15 |\n",
    "|2 | df3 | VIF variable set| 808993.20 | 899.44 | -0.123 | -0.269 | 186 | 7 |\n",
    "|2 | df3 | VIFplus variable set | 830910.29 | 911.54 | -0.154 | -0.353 | 186 | 9 |\n",
    "|3 | df6 | First variable set | 641315.04 | 800.82 | -0.084 | -0.224 | 399 | 15 |\n",
    "|3 | df6 | VIF variable set | 614096.13 | 783.64 | -0.038 | -0.097 | 399 | 7 |\n",
    "|3 | df6 | VIFplus variable set | 621438.85 | 788.31 | -0.050 | -0.128 | 399 | 9 |\n",
    "|4 | df9 |First variable set | 707250.58 | 840.98 | -0.212 | -0.347 | 456 | 15 |\n",
    "|4 | df9 | VIF variable set | 653417.75 | 808.34 | -0.120 | -0.175 | 456 | 7 |\n",
    "|4 | df9 | VIFplus variable set | 698990.70 | 836.06 | -0.198 | -0.274 | 456 | 9 |\n",
    "|5 | df23 | First variable set | 555730.01 | 745.47 | 0.055 | -0.036 | 517 | 15 |\n",
    "|5 | df23 | VIF variable set| 502591.44 | 708.94 | 0.146 | 0.109 | 517 | 7 |\n",
    "|5 | df23 | VIFplus variable set | 514135.96 | 717.03 | 0.126 | 0.077 | 517 | 9 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa49b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe the data\n",
    "#X1 = df2[x]\n",
    "#print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37e8d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(cols1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35a6f9d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count NAs per columns to check that step above worked #mat 607 before, now 0\n",
    "#X1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3e104b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b80b06",
   "metadata": {},
   "source": [
    "### Models without records that have disturbance matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ffc814",
   "metadata": {},
   "source": [
    "When dealing with positive forest EVI breaks, I can't remove records matched to Hansen et al.'s disturbance data as there are no such matched records. This makes sense as positive breaks should not be matched to disturbances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6ac6920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Df0: all records\n",
    "#df0 = df11 # N=687\n",
    "df0 = df0.loc[df0['hansen_year'].isnull()]\n",
    "# Df3: 1.5 months\n",
    "#df3 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.1315068) & (df11['magnitude']> 700)] #N=40\n",
    "df3 = df3.loc[df3['hansen_year'].isnull()]\n",
    "# Df6: 3 months\n",
    "#df6 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.2630137) & (df11['magnitude']> 700)] #N=168\n",
    "df6 = df6.loc[df6['hansen_year'].isnull()] #N=168\n",
    "# Df9: 5 months, version 4\n",
    "#df9 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.3945205) & (df11['magnitude']> 700)] #N=649\n",
    "df9 = df9.loc[df9['hansen_year'].isnull()] \n",
    "# Df23: 12 months, 1 year, version 5\n",
    "#df23 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 1.008219) & (df11['magnitude']> 700)] #N=2216\n",
    "df23 = df23.loc[df23['hansen_year'].isnull()] \n",
    "#dfall = [[df0],[df3], [df6], [df9], [df23]]\n",
    "dfall = [df0, df3, df6, df9, df23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31116268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many records are matched to disturbance data?\n",
    "#print(df2[['hansen_year']].describe()) # N=2775\n",
    "#print(df2[['magnitude', 'fire_year', 'harv_year', 'canlad_year', 'hansen_year']].describe()) # Hansen=? with df4; Hansen=648 with df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd6ee948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df2[['canlad_year']].describe()) # 2483; 247\n",
    "#print(df2[['harv_year']].describe()) # 1187; 204\n",
    "#print(df2[['fire_year']].describe()) # 139; 107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfcfc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This produces an empty df as there are no records\n",
    "#df3 = df2.drop(df2[df2.hansen_year > 0].index)\n",
    "#X3.tail\n",
    "#X3.shape\n",
    "#df3.describe()\n",
    "#df2.drop(df2[df2.hansen_year > 0].index, inplace=True) # gives a warning\n",
    "#df2.shape\n",
    "\n",
    "#df2 = df2.loc[df2['hansen_year'] > 0] # 2775\n",
    "\n",
    "#df2 = df2.loc[df2['hansen_year'].isnull()]\n",
    "\n",
    "#df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5188aab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304, 158)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfall[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea2bf42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.drop(df2[df2.hansen_year > 0].index).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "749bc1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1 | df0 | 15 | 741017.34 | 860.82 | -0.059 | -0.175 | 462 | 15 |\n",
      "| 1 | df0 | 7 | 750625.37 | 866.39 | -0.073 | -0.125 | 462 | 7 |\n",
      "| 1 | df0 | 9 | 763219.42 | 873.62 | -0.091 | -0.159 | 462 | 9 |\n",
      "| 2 | df3 | 15 | 1092545.89 | 1045.25 | -0.440 | -1.095 | 148 | 15 |\n",
      "| 2 | df3 | 7 | 980581.80 | 990.24 | -0.293 | -0.513 | 148 | 7 |\n",
      "| 2 | df3 | 9 | 1025797.41 | 1012.82 | -0.352 | -0.664 | 148 | 9 |\n",
      "| 3 | df6 | 15 | 559698.77 | 748.13 | -0.232 | -0.482 | 270 | 15 |\n",
      "| 3 | df6 | 7 | 517625.64 | 719.46 | -0.140 | -0.237 | 270 | 7 |\n",
      "| 3 | df6 | 9 | 543683.82 | 737.35 | -0.197 | -0.332 | 270 | 9 |\n",
      "| 4 | df9 | 15 | 678802.71 | 823.89 | -0.251 | -0.472 | 304 | 15 |\n",
      "| 4 | df9 | 7 | 621160.33 | 788.14 | -0.145 | -0.231 | 304 | 7 |\n",
      "| 4 | df9 | 9 | 652711.99 | 807.91 | -0.203 | -0.322 | 304 | 9 |\n",
      "| 5 | df23 | 15 | 679561.56 | 824.36 | -0.065 | -0.227 | 348 | 15 |\n",
      "| 5 | df23 | 7 | 565759.97 | 752.17 | 0.113 | 0.055 | 348 | 7 |\n",
      "| 5 | df23 | 9 | 624592.63 | 790.31 | 0.021 | -0.063 | 348 | 9 |\n"
     ]
    }
   ],
   "source": [
    "# loop version\n",
    "\n",
    "cols1 = ['for_age', 'for_con', 'map', 'map_lag1', 'map_lag2', 'map_lag3', 'mat', 'mat_lag1', 'mat_lag2', 'mat_lag3', 'rh', 'rh_lag1', 'rh_lag2', 'rh_lag3', 'for_pro_0']\n",
    "cols2 = ['for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt_lag1', 'dd5_wt_lag3']\n",
    "cols3 = ['for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt_lag1', 'dd5_wt_lag3', 'for_age', 'for_pro_0']\n",
    "\n",
    "df_labs = ['df0','df3', 'df6', 'df9', 'df23']\n",
    "\n",
    "for z in range(len(dfall)):\n",
    "    list_of_vars = [ [cols1], [cols2], [cols3]]\n",
    "    for list in list_of_vars:\n",
    "        for x in list:\n",
    "            #print(x)\n",
    "            X1 = dfall[z][x]\n",
    "            #print(X1.describe())\n",
    "            y1 = dfall[z].iloc[:,6].abs()\n",
    "            seed = 7 # random seed to help with replication\n",
    "            testsize1 = 0.33 # percent of records to test after training\n",
    "            x1_train, x1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=testsize1, random_state=seed) # Split data set. Note the 'stratify' option\n",
    "            model_bp2 = XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
    "                 colsample_bylevel=None, colsample_bynode=None,\n",
    "                 colsample_bytree=None, early_stopping_rounds=50,\n",
    "                 enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "                 gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "                 interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
    "                 max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "                 max_delta_step=None, max_depth=8, max_leaves=None,\n",
    "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "                 n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
    "                 predictor=None, random_state=42, reg_lambda=10, reg_alpha=1)\n",
    "               # EVALUATION (with test)\n",
    "            eval_set = [(x1_train, y1_train), (x1_test, y1_test)]\n",
    "                #UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
    "            model_bp2.fit(x1_train, y1_train, eval_set=eval_set, verbose=False)\n",
    "                # make predictions for test data\n",
    "            y_pred = model_bp2.predict(x1_test)\n",
    "            predictions = [round(value) for value in y_pred]\n",
    "                # retrieve performance metrics\n",
    "            results = model_bp2.evals_result()\n",
    "            mse = mean_squared_error(y1_test, y_pred)\n",
    "                #r2 = explained_variance_score(y1_test, ypred)\n",
    "            r2 = r2_score(y1_test, y_pred)\n",
    "                # adjusted R-squared\n",
    "            adj_r2 = 1 - (((1-r2) * (len(y1_test)-1))/(len(y1_test)-x1_test.shape[1]-1))\n",
    "\n",
    "            #print(\"MSE: %.2f\" % mse)\n",
    "\n",
    "            #print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
    "\n",
    "            #print(\"R-sq: %.3f\" % r2)\n",
    "\n",
    "            #print(\"R-sq-adj: %.3f\" % adj_r2)\n",
    "\n",
    "             # row for table\n",
    "            #print(\"| %.2f\" % mse, \"| %.2f\" % (mse**(1/2.0)), \"| %.3f\" % r2, \"| %.3f\" % adj_r2, \"|\", X1.shape[0], \"|\", X1.shape[1],\"|\")\n",
    "            print(\"|\", z+1, \"|\", df_labs[z], \"|\", len(x), \"| %.2f\" % mse, \"| %.2f\" % (mse**(1/2.0)), \"| %.3f\" % r2, \"| %.3f\" % adj_r2, \"|\", X1.shape[0], \"|\", X1.shape[1],\"|\")\n",
    "\n",
    "            # Feature importance plot\n",
    "            #xgb.plot_importance(model_bp2, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance, gain', \n",
    "            #            xlabel='F score - Gain', ylabel='Features', \n",
    "            #            importance_type='gain', max_num_features=15, grid=True, show_values=False) #, values_format='{v:.2f}' )\n",
    "\n",
    "            #pyplot.savefig(r'.\\figs\\version4\\h2p\\df23\\neg_gain_m{y}_v2.png'.format(y=len(x)),  dpi=300, bbox_inches='tight')\n",
    "            #pyplot.show()\n",
    "            # create lis of feature names to be used in dependency plot so that high ranking vars are plotted\n",
    "            #features_names1 = pd.DataFrame()\n",
    "            #features_names1['columns'] = X1.columns\n",
    "            #features_names1['importances'] = model_bp2.feature_importances_\n",
    "            #features_names1.sort_values(by='importances',ascending=False,inplace=True)\n",
    "            #features_names2 = features_names1['columns'].tolist()[0:10]\n",
    "\n",
    "            #_, ax1 = plt.subplots(figsize=(9, 8), constrained_layout=True)\n",
    "\n",
    "            #display = PartialDependenceDisplay.from_estimator(model_bp2, x1_train, features_names2, ax=ax1)\n",
    "\n",
    "            #_ = display.figure_.suptitle((\"Partial dependence plots\"), fontsize=12, )\n",
    "\n",
    "            #pyplot.savefig(r'.\\figs\\version4\\h2p\\df23\\neg_partial_dep_m{y}_v2.png'.format(y=len(x)),  dpi=300, bbox_inches='tight')\n",
    "\n",
    "            #pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe781bf",
   "metadata": {},
   "source": [
    "Partial dependence plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd7d5a9",
   "metadata": {},
   "source": [
    "**Table 2**: Model comparison for negative breaks. Subset of data records was used, excluding records that had a match with disturbance data. (Some NAs for for_age and for_con.)\n",
    "\n",
    "|ID| Data frame| Model   | MSE| RMSE| R-sq | R-sq-adj | N rows| N vars|\n",
    "| --------| --------| -------- | ------- |-------- | ------- |------- |------- |------- |\n",
    "| 1 | df0 | First variable set | 741017.34 | 860.82 | -0.059 | -0.175 | 462 | 15 |\n",
    "| 2 | df0 | VIF variable set | 750625.37 | 866.39 | -0.073 | -0.125 | 462 | 7 |\n",
    "| 3 | df0 | VIFplus variable set | 763219.42 | 873.62 | -0.091 | -0.159 | 462 | 9 |\n",
    "| 4 | df3 | First variable set | 1092545.89 | 1045.25 | -0.440 | -1.095 | 148 | 15 |\n",
    "| 5 | df3 | VIF variable set | 980581.80 | 990.24 | -0.293 | -0.513 | 148 | 7 |\n",
    "| 6 | df3 | VIFplus variable set | 1025797.41 | 1012.82 | -0.352 | -0.664 | 148 | 9 |\n",
    "| 7 | df6 | First variable set | 559698.77 | 748.13 | -0.232 | -0.482 | 270 | 15 |\n",
    "| 8 | df6 | VIF variable set | 517625.64 | 719.46 | -0.140 | -0.237 | 270 | 7 |\n",
    "| 9 | df6 | VIFplus variable set | 543683.82 | 737.35 | -0.197 | -0.332 | 270 | 9 |\n",
    "| 10 | df9 | First variable set | 678802.71 | 823.89 | -0.251 | -0.472 | 304 | 15 |\n",
    "| 11 | df9 | VIF variable set | 621160.33 | 788.14 | -0.145 | -0.231 | 304 | 7 |\n",
    "| 12 | df9 | VIFplus variable set | 652711.99 | 807.91 | -0.203 | -0.322 | 304 | 9 |\n",
    "| 13 | df23 | First variable set | 679561.56 | 824.36 | -0.065 | -0.227 | 348 | 15 |\n",
    "| 14 | df23 | VIF variable set | 565759.97 | 752.17 | 0.113 | 0.055 | 348 | 7 |\n",
    "| 15 | df23 | VIFplus variable set | 624592.63 | 790.31 | 0.021 | -0.063 | 348 | 9 |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "020b1743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(348, 9)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check one more\n",
    "#df2.shape\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec885f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
