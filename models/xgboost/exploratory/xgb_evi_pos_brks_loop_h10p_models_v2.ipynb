{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9dab20",
   "metadata": {},
   "source": [
    "# Positive breaks XGBoost model - local run with loop\n",
    "\n",
    "### Peter R.\n",
    "#### 2024-03-14\n",
    "\n",
    "Note: Here I use h10p, that is Bfast breaks with h=0.1 or ~ 10%. The filtering of records with h10p is the samet than with h5p.\n",
    "\n",
    "Today (2024-03-04) I meet with MJF to discuss XGB model improvements. An important number of Bfast breaks have very wide confidence intervals (CIs) associated with the time of break. These CIs can range from about 1 month to 80+ months. These won't allow for high quality matching with yearly climate or disturbance data. For this reason, we decided to run XGB models with subsets of data, each subset has narrow CIs. We will run the following XGB models with the follwoing dataframe subsets:\n",
    "\n",
    "- Dataframe3 (df3): Records with CIs shorter than 3 16-days data points (48 days or about 1.5 months)\n",
    "\n",
    "- Dataframe6 (df6): Records with CIs shorter than 6 16-days data points (96 days or about 3 months)\n",
    "\n",
    "- Dataframe9 (df9): Records with CIs shorter than 9 16-days data points (144 days or about 5 months)\n",
    "\n",
    "- Dataframe23 (df23): Records with CIS shorter than 23 16-days data points (368 days or about 1 year)\n",
    "\n",
    "Some questions to have in mind:\n",
    "\n",
    "- How many matches with disturbance data do the above have?\n",
    "- Why does forest age become the top ranking variable with VIFplust variable set?  This variable was number 10 in other previous XGB model.\n",
    "- I am assuming that Hansen is best and that it only includes stand-replacing disturbances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ed8877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB version: 1.7.6\n",
      "negative breaks\n"
     ]
    }
   ],
   "source": [
    "# 2024-03-12\n",
    "# Peter R.\n",
    "# XGBoost script\n",
    "# Positive breaks, n_estimators (number of trees)=1000 and with optimal parameter from DRAC model_bp1 & early stopping\n",
    "\n",
    "#Here I am using a loop to run several models at a time\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import nan\n",
    "import xgboost as xgb\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# for feature importance plots\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#for dependency plots\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "#start = time.time()\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#print(cwd)\n",
    "\n",
    "# DRAC directory\n",
    "#os.chdir(\"/home/georod/projects/def-mfortin/georod/scripts/github/forc_trends/models/xgboost\")\n",
    "# Win directory\n",
    "os.chdir(r'C:\\Users\\Peter R\\github\\forc_trends\\models\\xgboost')\n",
    "\n",
    "\n",
    "print(\"XGB version:\", xgb.__version__)\n",
    "print(\"negative breaks\")\n",
    "\n",
    "\n",
    "# Windows\n",
    "df1 = pd.read_csv(r'.\\data\\forest_evi_breaks_positive_h10p_v3.csv', skipinitialspace=True)\n",
    "# DRAC\n",
    "#df1 = pd.read_csv(r'./data/forest_evi_breaks_positive_v2.csv', skipinitialspace=True)\n",
    "#df1.head()\n",
    "\n",
    "\n",
    "df11 = pd.get_dummies(df1, columns=['for_pro'], dtype=float)\n",
    "\n",
    "#Df0: all rows\n",
    "#df2 = df11 # N=843\n",
    "# Df3: 1.5 months, version4/df3\n",
    "#df2 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.1315068) & (df11['magnitude'] < -700)] #N= 171\n",
    "# Df6: 3 months, version4/df6\n",
    "#df2 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.2630137) & (df11['magnitude'] < -700)] #N= 450\n",
    "# Df9: 5 months, version4/df9\n",
    "# df2 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.3945205) & (df11['magnitude']< -700)] #N=523\n",
    "# Df23: 12 months, 1 year, version4/df23\n",
    "#df2 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 1.008219) & (df11['magnitude'] < -700)] #N=649\n",
    "\n",
    "# Df23v2: 12 months, 1 year, bounded by same year, version4/df23v2\n",
    "#df2 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 1.008219) & (df11['magnitude'] < -700)] # N=2291\n",
    "#df2 = df2.loc[(np.floor(df2['brkdate95'])==np.floor(df2['brkdate25']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a599c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Df0: all rows\n",
    "df0 = df11 # N=5073\n",
    "# Df3: 1.5 months, version4/df3\n",
    "df3 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.09) & (df11['magnitude'] > 500)] #N= 0, 0.1315068\n",
    "#Df6: 3 months, version4/df6\n",
    "df6 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.2630137) & (df11['magnitude'] > 500)] #N= 205\n",
    "# Df9: 5 months, version4/df9\n",
    "df9 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.3945205) & (df11['magnitude'] > 500)] #N=487\n",
    "# Df23: 12 months, 1 year, version4/df23\n",
    "df23 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 1.008219) & (df11['magnitude'] > 500)] #N=5347\n",
    "\n",
    "#dfall = [df0,df3, df6, df9, df23]\n",
    "dfall = [df0, df6, df9, df23]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9078a73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 pix         year     brk    brkdate25      brkdate  \\\n",
      "count    1185.000000  1185.000000  1185.0  1185.000000  1185.000000   \n",
      "mean   228208.130802  2013.899578     0.0  2014.206620  2014.561610   \n",
      "std    139166.964025     5.658276     0.0     5.601179     5.751487   \n",
      "min       608.000000  2005.000000     0.0  2004.957000  2005.000000   \n",
      "25%    117074.000000  2009.000000     0.0  2008.913000  2009.000000   \n",
      "50%    220618.000000  2014.000000     0.0  2014.696000  2014.957000   \n",
      "75%    331498.000000  2020.000000     0.0  2020.217000  2020.826000   \n",
      "max    493282.000000  2020.000000     0.0  2020.826000  2020.957000   \n",
      "\n",
      "         brkdate95    magnitude  no_brk  fire_year    harv_year  ...  \\\n",
      "count  1185.000000  1185.000000     0.0        0.0    30.000000  ...   \n",
      "mean   2014.923141   765.512148     NaN        NaN  2010.200000  ...   \n",
      "std       5.617967   181.518861     NaN        NaN     5.040525  ...   \n",
      "min    2005.304000   500.471000     NaN        NaN  2005.000000  ...   \n",
      "25%    2009.652000   627.874000     NaN        NaN  2006.000000  ...   \n",
      "50%    2015.348000   734.490000     NaN        NaN  2009.000000  ...   \n",
      "75%    2021.000000   876.582000     NaN        NaN  2013.750000  ...   \n",
      "max    2021.826000  1812.526000     NaN        NaN  2020.000000  ...   \n",
      "\n",
      "              bffp    bffp_lag1    bffp_lag2    bffp_lag3         effp  \\\n",
      "count  1185.000000  1185.000000  1185.000000  1185.000000  1185.000000   \n",
      "mean    143.126723   145.215511   145.067505   141.724246   267.379617   \n",
      "std       4.613439     5.776171     4.223398     3.465962     4.575524   \n",
      "min     129.248000   129.924000   129.168000   129.924000   261.000000   \n",
      "25%     140.000000   140.966000   142.252000   139.898000   263.718000   \n",
      "50%     144.000000   145.047000   146.000000   141.125000   266.000000   \n",
      "75%     146.841000   150.549000   148.000000   143.622000   271.954000   \n",
      "max     154.000000   154.000000   155.000000   154.000000   279.034000   \n",
      "\n",
      "         effp_lag1    effp_lag2    effp_lag3    for_pro_0    for_pro_1  \n",
      "count  1185.000000  1185.000000  1185.000000  1185.000000  1185.000000  \n",
      "mean    268.157472   268.130941   270.827454     0.910549     0.089451  \n",
      "std       3.470864     3.246682     4.810780     0.285515     0.285515  \n",
      "min     261.000000   261.187000   261.000000     0.000000     0.000000  \n",
      "25%     265.192000   266.469000   267.000000     1.000000     0.000000  \n",
      "50%     268.000000   268.000000   272.000000     1.000000     0.000000  \n",
      "75%     271.000000   269.576000   275.000000     1.000000     0.000000  \n",
      "max     279.000000   280.569000   280.982000     1.000000     1.000000  \n",
      "\n",
      "[8 rows x 158 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dfall[3].describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45deb38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix</th>\n",
       "      <th>year</th>\n",
       "      <th>brk</th>\n",
       "      <th>brkdate25</th>\n",
       "      <th>brkdate</th>\n",
       "      <th>brkdate95</th>\n",
       "      <th>magnitude</th>\n",
       "      <th>no_brk</th>\n",
       "      <th>fire_year</th>\n",
       "      <th>harv_year</th>\n",
       "      <th>...</th>\n",
       "      <th>bffp</th>\n",
       "      <th>bffp_lag1</th>\n",
       "      <th>bffp_lag2</th>\n",
       "      <th>bffp_lag3</th>\n",
       "      <th>effp</th>\n",
       "      <th>effp_lag1</th>\n",
       "      <th>effp_lag2</th>\n",
       "      <th>effp_lag3</th>\n",
       "      <th>for_pro_0</th>\n",
       "      <th>for_pro_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>195275</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>2006.696</td>\n",
       "      <td>2006.739</td>\n",
       "      <td>2006.957</td>\n",
       "      <td>527.986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>135.000</td>\n",
       "      <td>151.00</td>\n",
       "      <td>145.000</td>\n",
       "      <td>265.0</td>\n",
       "      <td>273.000</td>\n",
       "      <td>268.000</td>\n",
       "      <td>267.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>44203</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>2007.609</td>\n",
       "      <td>2007.783</td>\n",
       "      <td>2007.826</td>\n",
       "      <td>661.731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>137.0</td>\n",
       "      <td>137.000</td>\n",
       "      <td>129.98</td>\n",
       "      <td>147.087</td>\n",
       "      <td>276.0</td>\n",
       "      <td>267.913</td>\n",
       "      <td>276.913</td>\n",
       "      <td>270.913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>346407</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>2014.304</td>\n",
       "      <td>2014.348</td>\n",
       "      <td>2014.522</td>\n",
       "      <td>1310.899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>137.0</td>\n",
       "      <td>140.000</td>\n",
       "      <td>136.00</td>\n",
       "      <td>137.000</td>\n",
       "      <td>275.0</td>\n",
       "      <td>268.000</td>\n",
       "      <td>272.000</td>\n",
       "      <td>275.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>1873</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>2008.522</td>\n",
       "      <td>2008.565</td>\n",
       "      <td>2008.739</td>\n",
       "      <td>770.015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>145.0</td>\n",
       "      <td>143.156</td>\n",
       "      <td>143.00</td>\n",
       "      <td>138.000</td>\n",
       "      <td>262.0</td>\n",
       "      <td>272.000</td>\n",
       "      <td>263.000</td>\n",
       "      <td>272.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>1875</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>2008.565</td>\n",
       "      <td>2008.609</td>\n",
       "      <td>2008.783</td>\n",
       "      <td>725.291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>145.0</td>\n",
       "      <td>143.000</td>\n",
       "      <td>143.00</td>\n",
       "      <td>137.620</td>\n",
       "      <td>262.0</td>\n",
       "      <td>272.000</td>\n",
       "      <td>263.000</td>\n",
       "      <td>272.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pix  year  brk  brkdate25   brkdate  brkdate95  magnitude  no_brk  \\\n",
       "123   195275  2006    0   2006.696  2006.739   2006.957    527.986     NaN   \n",
       "441    44203  2007    0   2007.609  2007.783   2007.826    661.731     NaN   \n",
       "1152  346407  2014    0   2014.304  2014.348   2014.522   1310.899     NaN   \n",
       "1170    1873  2008    0   2008.522  2008.565   2008.739    770.015     NaN   \n",
       "1171    1875  2008    0   2008.565  2008.609   2008.783    725.291     NaN   \n",
       "\n",
       "      fire_year  harv_year  ...   bffp  bffp_lag1  bffp_lag2  bffp_lag3  \\\n",
       "123         NaN        NaN  ...  140.0    135.000     151.00    145.000   \n",
       "441         NaN        NaN  ...  137.0    137.000     129.98    147.087   \n",
       "1152        NaN        NaN  ...  137.0    140.000     136.00    137.000   \n",
       "1170        NaN        NaN  ...  145.0    143.156     143.00    138.000   \n",
       "1171        NaN        NaN  ...  145.0    143.000     143.00    137.620   \n",
       "\n",
       "       effp  effp_lag1  effp_lag2  effp_lag3  for_pro_0  for_pro_1  \n",
       "123   265.0    273.000    268.000    267.000        1.0        0.0  \n",
       "441   276.0    267.913    276.913    270.913        1.0        0.0  \n",
       "1152  275.0    268.000    272.000    275.000        1.0        0.0  \n",
       "1170  262.0    272.000    263.000    272.000        1.0        0.0  \n",
       "1171  262.0    272.000    263.000    272.000        1.0        0.0  \n",
       "\n",
       "[5 rows x 158 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dfall[1]).head()\n",
    "#range(len(dfall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57147b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1 | df0 | First variable set | 24790.41 | 157.45 | 0.243 | 0.236 | 5073 | 15 |\n",
      "| 1 | df0 | VIF variable set | 24217.77 | 155.62 | 0.260 | 0.257 | 5073 | 7 |\n",
      "| 1 | df0 | VIFplus variable set | 24872.98 | 157.71 | 0.240 | 0.236 | 5073 | 9 |\n",
      "| 2 | df3 | First variable set | 60622.99 | 246.22 | -0.337 | 2.528 | 25 | 15 |\n",
      "| 2 | df3 | VIF variable set | 55962.67 | 236.56 | -0.234 | -8.875 | 25 | 7 |\n",
      "| 2 | df3 | VIFplus variable set | 60842.01 | 246.66 | -0.342 | 11.736 | 25 | 9 |\n",
      "| 3 | df6 | First variable set | 46846.05 | 216.44 | 0.139 | -1.036 | 80 | 15 |\n",
      "| 3 | df6 | VIF variable set | 66782.91 | 258.42 | -0.228 | -0.681 | 80 | 7 |\n",
      "| 3 | df6 | VIFplus variable set | 63568.00 | 252.13 | -0.169 | -0.788 | 80 | 9 |\n",
      "| 4 | df9 | First variable set | 25140.81 | 158.56 | 0.176 | 0.143 | 1185 | 15 |\n",
      "| 4 | df9 | VIF variable set | 26888.44 | 163.98 | 0.119 | 0.103 | 1185 | 7 |\n",
      "| 4 | df9 | VIFplus variable set | 25794.95 | 160.61 | 0.154 | 0.135 | 1185 | 9 |\n"
     ]
    }
   ],
   "source": [
    "# loop version\n",
    "\n",
    "cols1 = ['for_age', 'for_con', 'map', 'map_lag1', 'map_lag2', 'map_lag3', 'mat', 'mat_lag1', 'mat_lag2', 'mat_lag3', 'rh', 'rh_lag1', 'rh_lag2', 'rh_lag3', 'for_pro_0']\n",
    "cols2 = ['for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt_lag1', 'dd5_wt_lag3']\n",
    "cols3 = ['for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt_lag1', 'dd5_wt_lag3', 'for_age', 'for_pro_0']\n",
    "\n",
    "df_labs = ['df0','df3', 'df6', 'df9', 'df23']\n",
    "model_labs = ['First variable set', 'VIF variable set', 'VIFplus variable set']\n",
    "\n",
    "for z in range(len(dfall)):\n",
    "    list_of_vars = [[cols1], [cols2], [cols3]]\n",
    "    for index, list in enumerate(list_of_vars):\n",
    "        for x in list:\n",
    "            #print(x)\n",
    "            X1 = dfall[z][x]\n",
    "            #print(X1.describe())\n",
    "            y1 = dfall[z].iloc[:,6].abs()\n",
    "            seed = 7 # random seed to help with replication\n",
    "            testsize1 = 0.33 # percent of records to test after training\n",
    "            x1_train, x1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=testsize1, random_state=seed) # Split data set. Note the 'stratify' option\n",
    "            model_bp2 = XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
    "                 colsample_bylevel=None, colsample_bynode=None,\n",
    "                 colsample_bytree=None, early_stopping_rounds=50,\n",
    "                 enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "                 gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "                 interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
    "                 max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "                 max_delta_step=None, max_depth=8, max_leaves=None,\n",
    "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "                 n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
    "                 predictor=None, random_state=42, reg_lambda=10, reg_alpha=1)\n",
    "               # EVALUATION (with test)\n",
    "            eval_set = [(x1_train, y1_train), (x1_test, y1_test)]\n",
    "                #UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
    "            model_bp2.fit(x1_train, y1_train, eval_set=eval_set, verbose=False)\n",
    "                # make predictions for test data\n",
    "            y_pred = model_bp2.predict(x1_test)\n",
    "            predictions = [round(value) for value in y_pred]\n",
    "                # retrieve performance metrics\n",
    "            results = model_bp2.evals_result()\n",
    "            mse = mean_squared_error(y1_test, y_pred)\n",
    "                #r2 = explained_variance_score(y1_test, ypred)\n",
    "            r2 = r2_score(y1_test, y_pred)\n",
    "                # adjusted R-squared\n",
    "            adj_r2 = 1 - (((1-r2) * (len(y1_test)-1))/(len(y1_test)-x1_test.shape[1]-1))\n",
    "\n",
    "            #print(\"MSE: %.2f\" % mse)\n",
    "            var1 = \"%.2f\" % mse\n",
    "\n",
    "            #print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
    "            var2 = \"%.2f\" % (mse**(1/2.0))\n",
    "\n",
    "            #print(\"R-sq: %.3f\" % r2)\n",
    "            var3 = \"%.3f\" % r2\n",
    "\n",
    "            #print(\"R-sq-adj: %.3f\" % adj_r2)\n",
    "            var4 = \"%.3f\" % adj_r2\n",
    "\n",
    "            var5 = X1.shape[0]\n",
    "            var6 = X1.shape[1]\n",
    "\n",
    "            # row for table\n",
    "            #print(\"| %.2f\" % mse, \"| %.2f\" % (mse**(1/2.0)), \"| %.3f\" % r2, \"| %.3f\" % adj_r2, \"|\", X1.shape[0], \"|\", X1.shape[1],\"|\")\n",
    "            #print(\"|\", z+1, \"|\", df_labs[z], \"|\", model_labs[index], \"| %.2f\" % mse, \"| %.2f\" % (mse**(1/2.0)), \"| %.3f\" % r2, \"| %.3f\" % adj_r2, \"|\", X1.shape[0], \"|\", X1.shape[1],\"|\")\n",
    "            print(\"|\", (z+1), \"|\", df_labs[z], \"|\", model_labs[index], \"|\", var1, \"|\", var2, \"|\", var3, \"|\", var4, \"|\",var5, \"|\",var6, \"|\")\n",
    "            # Feature importance plot\n",
    "            #xgb.plot_importance(model_bp2, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance, gain', \n",
    "            #            xlabel='F score - Gain', ylabel='Features', \n",
    "            #            importance_type='gain', max_num_features=15, grid=True, show_values=False) #, values_format='{v:.2f}' )\n",
    "\n",
    "            #pyplot.savefig(r'.\\figs\\version4\\h2p\\df23\\neg_gain_m{y}_v1.png'.format(y=len(x)),  dpi=300, bbox_inches='tight')\n",
    "            #pyplot.show()\n",
    "            # create lis of feature names to be used in dependency plot so that high ranking vars are plotted\n",
    "            #features_names1 = pd.DataFrame()\n",
    "           # features_names1['columns'] = X1.columns\n",
    "           # features_names1['importances'] = model_bp2.feature_importances_\n",
    "           # features_names1.sort_values(by='importances',ascending=False,inplace=True)\n",
    "           # features_names2 = features_names1['columns'].tolist()[0:10]\n",
    "\n",
    "           # _, ax1 = plt.subplots(figsize=(9, 8), constrained_layout=True)\n",
    "\n",
    "           # display = PartialDependenceDisplay.from_estimator(model_bp2, x1_train, features_names2, ax=ax1)\n",
    "\n",
    "           # _ = display.figure_.suptitle((\"Partial dependence plots\"), fontsize=12, )\n",
    "\n",
    "           # pyplot.savefig(r'.\\figs\\version4\\h2p\\df23\\neg_partial_dep_m{y}_v1.png'.format(y=len(x)),  dpi=300, bbox_inches='tight')\n",
    "\n",
    "           # pyplot.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1ddf3",
   "metadata": {},
   "source": [
    "**Table 1**: Model comparison for negative breaks. Standard data set with all records (including NAs for for_age and for_con).\n",
    "\n",
    "\n",
    "|ID|Data frame| Model   | MSE| RMSE| R-sq | R-sq-adj |N rows| N vars|\n",
    "| --------| --------| --------| --------| -------- | ------- |-------- | ------- |------- |\n",
    "| 1 | df0 | First variable set | 24790.41 | 157.45 | 0.243 | 0.236 | 5073 | 15 |\n",
    "| 2 | df0 | VIF variable set | 24217.77 | 155.62 | 0.260 | 0.257 | 5073 | 7 |\n",
    "| 3 | df0 | VIFplus variable set | 24872.98 | 157.71 | 0.240 | 0.236 | 5073 | 9 |\n",
    "| 4 | df3 | First variable set | 60622.99 | 246.22 | -0.337 | 2.528 | 25 | 15 |\n",
    "| 5 | df3 | VIF variable set | 55962.67 | 236.56 | -0.234 | -8.875 | 25 | 7 |\n",
    "| 6 | df3 | VIFplus variable set | 60842.01 | 246.66 | -0.342 | 11.736 | 25 | 9 |\n",
    "| 7 | df6 | First variable set | 46846.05 | 216.44 | 0.139 | -1.036 | 80 | 15 |\n",
    "| 8 | df6 | VIF variable set | 66782.91 | 258.42 | -0.228 | -0.681 | 80 | 7 |\n",
    "| 9 | df6 | VIFplus variable set | 63568.00 | 252.13 | -0.169 | -0.788 | 80 | 9 |\n",
    "| 10 | df9 | First variable set | 25140.81 | 158.56 | 0.176 | 0.143 | 1185 | 15 |\n",
    "| 11 | df9 | VIF variable set | 26888.44 | 163.98 | 0.119 | 0.103 | 1185 | 7 |\n",
    "| 12 | df9 | VIFplus variable set | 25794.95 | 160.61 | 0.154 | 0.135 | 1185 | 9 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "35a6f9d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Misc cmds\n",
    "#Describe the data\n",
    "#X1 = df2[x]\n",
    "#print(X1.shape)\n",
    "#x\n",
    "#print(len(cols1))\n",
    "# Count NAs per columns to check that step above worked #mat 607 before, now 0\n",
    "#X1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e3e104b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b80b06",
   "metadata": {},
   "source": [
    "### Models without records that have disturbance matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ffc814",
   "metadata": {},
   "source": [
    "When dealing with positive forest EVI breaks, I can't remove records matched to Hansen et al.'s disturbance data as there are no such matched records. This makes sense as positive breaks should not be matched to disturbances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "31116268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many records are matched to disturbance data?\n",
    "#print(df2[['hansen_year']].describe()) # N=2775\n",
    "#print(df2[['magnitude', 'fire_year', 'harv_year', 'canlad_year', 'hansen_year']].describe()) # Hansen=? with df4; Hansen=648 with df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "28c2999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Df0: all records\n",
    "#df0 = df11 # N=687\n",
    "df0 = df0.loc[df0['hansen_year'].isnull()]\n",
    "# Df3: 1.5 months\n",
    "#df3 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.1315068) & (df11['magnitude']> 700)] #N=40\n",
    "df3 = df3.loc[df3['hansen_year'].isnull()]\n",
    "# Df6: 3 months\n",
    "#df6 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.2630137) & (df11['magnitude']> 700)] #N=168\n",
    "df6 = df6.loc[df6['hansen_year'].isnull()] #N=168\n",
    "# Df9: 5 months, version 4\n",
    "#df9 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.3945205) & (df11['magnitude']> 700)] #N=649\n",
    "df9 = df9.loc[df9['hansen_year'].isnull()] \n",
    "# Df23: 12 months, 1 year, version 5\n",
    "#df23 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 1.008219) & (df11['magnitude']> 700)] #N=2216\n",
    "df23 = df23.loc[df23['hansen_year'].isnull()] \n",
    "#dfall = [[df0],[df3], [df6], [df9], [df23]]\n",
    "#dfall = [df0, df3, df6, df9, df23]\n",
    "dfall = [df0, df6, df9, df23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dd6ee948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df2[['canlad_year']].describe()) # 2483; 247\n",
    "#print(df2[['harv_year']].describe()) # 1187; 204\n",
    "#print(df2[['fire_year']].describe()) # 139; 107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bfcfc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This produces an empty df as there are no records\n",
    "#df3 = df2.drop(df2[df2.hansen_year > 0].index)\n",
    "#X3.tail\n",
    "#X3.shape\n",
    "#df3.describe()\n",
    "#df2.drop(df2[df2.hansen_year > 0].index, inplace=True) # gives a warning\n",
    "#df2.shape\n",
    "\n",
    "#df2 = df2.loc[df2['hansen_year'] > 0] # 2775\n",
    "\n",
    "#df2 = df2.loc[df2['hansen_year'].isnull()]\n",
    "\n",
    "#df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ea2bf42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.drop(df2[df2.hansen_year > 0].index).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4baf9489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 158)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfall[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "749bc1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1 | df0 | First variable set | 23664.83 | 153.83 | 0.386 | 0.383 | 11118 | 15 |\n",
      "| 1 | df0 | VIF variable set | 23734.41 | 154.06 | 0.384 | 0.383 | 11118 | 7 |\n",
      "| 1 | df0 | VIFplus variable set | 24011.27 | 154.96 | 0.377 | 0.375 | 11118 | 9 |\n",
      "| 2 | df6 | First variable set | 25736.86 | 160.43 | 0.652 | 1.126 | 15 | 15 |\n",
      "| 2 | df6 | VIF variable set | 32191.19 | 179.42 | 0.565 | 1.580 | 15 | 7 |\n",
      "| 2 | df6 | VIFplus variable set | 32191.19 | 179.42 | 0.565 | 1.348 | 15 | 9 |\n",
      "| 3 | df9 | First variable set | 45051.81 | 212.25 | 0.606 | 0.070 | 81 | 15 |\n",
      "| 3 | df9 | VIF variable set | 55276.32 | 235.11 | 0.517 | 0.339 | 81 | 7 |\n",
      "| 3 | df9 | VIFplus variable set | 55457.17 | 235.49 | 0.515 | 0.259 | 81 | 9 |\n",
      "| 4 | df23 | First variable set | 28267.67 | 168.13 | 0.290 | 0.276 | 2425 | 15 |\n",
      "| 4 | df23 | VIF variable set | 28986.44 | 170.25 | 0.272 | 0.265 | 2425 | 7 |\n",
      "| 4 | df23 | VIFplus variable set | 28023.82 | 167.40 | 0.296 | 0.288 | 2425 | 9 |\n"
     ]
    }
   ],
   "source": [
    "# loop version\n",
    "\n",
    "cols1 = ['for_age', 'for_con', 'map', 'map_lag1', 'map_lag2', 'map_lag3', 'mat', 'mat_lag1', 'mat_lag2', 'mat_lag3', 'rh', 'rh_lag1', 'rh_lag2', 'rh_lag3', 'for_pro_0']\n",
    "cols2 = ['for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt_lag1', 'dd5_wt_lag3']\n",
    "cols3 = ['for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt_lag1', 'dd5_wt_lag3', 'for_age', 'for_pro_0']\n",
    "\n",
    "#df_labs = ['df0','df3', 'df6', 'df9', 'df23']\n",
    "df_labs = ['df0', 'df6','df9', 'df23']\n",
    "model_labs = ['First variable set', 'VIF variable set', 'VIFplus variable set']\n",
    "\n",
    "for z in range(len(dfall)):\n",
    "    list_of_vars = [ [cols1], [cols2], [cols3]]\n",
    "    for index, list in enumerate(list_of_vars):\n",
    "        for x in list:\n",
    "            #print(x)\n",
    "            X1 = dfall[z][x]\n",
    "            #print(X1.describe())\n",
    "            y1 = dfall[z].iloc[:,6].abs()\n",
    "            seed = 7 # random seed to help with replication\n",
    "            testsize1 = 0.33 # percent of records to test after training\n",
    "            x1_train, x1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=testsize1, random_state=seed) # Split data set. Note the 'stratify' option\n",
    "            model_bp2 = XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
    "                 colsample_bylevel=None, colsample_bynode=None,\n",
    "                 colsample_bytree=None, early_stopping_rounds=50,\n",
    "                 enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "                 gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "                 interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
    "                 max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "                 max_delta_step=None, max_depth=8, max_leaves=None,\n",
    "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "                 n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
    "                 predictor=None, random_state=42, reg_lambda=10, reg_alpha=1)\n",
    "               # EVALUATION (with test)\n",
    "            eval_set = [(x1_train, y1_train), (x1_test, y1_test)]\n",
    "                #UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
    "            model_bp2.fit(x1_train, y1_train, eval_set=eval_set, verbose=False)\n",
    "                # make predictions for test data\n",
    "            y_pred = model_bp2.predict(x1_test)\n",
    "            predictions = [round(value) for value in y_pred]\n",
    "                # retrieve performance metrics\n",
    "            results = model_bp2.evals_result()\n",
    "            mse = mean_squared_error(y1_test, y_pred)\n",
    "                #r2 = explained_variance_score(y1_test, ypred)\n",
    "            r2 = r2_score(y1_test, y_pred)\n",
    "                # adjusted R-squared\n",
    "            adj_r2 = 1 - (((1-r2) * (len(y1_test)-1))/(len(y1_test)-x1_test.shape[1]-1))\n",
    "\n",
    "            #print(\"MSE: %.2f\" % mse)\n",
    "            var1 = \"%.2f\" % mse\n",
    "\n",
    "            #print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
    "            var2 = \"%.2f\" % (mse**(1/2.0))\n",
    "\n",
    "            #print(\"R-sq: %.3f\" % r2)\n",
    "            var3 = \"%.3f\" % r2\n",
    "\n",
    "            #print(\"R-sq-adj: %.3f\" % adj_r2)\n",
    "            var4 = \"%.3f\" % adj_r2\n",
    "\n",
    "            var5 = X1.shape[0]\n",
    "            var6 = X1.shape[1]\n",
    "\n",
    "             # row for table\n",
    "            #print(\"| %.2f\" % mse, \"| %.2f\" % (mse**(1/2.0)), \"| %.3f\" % r2, \"| %.3f\" % adj_r2, \"|\", X1.shape[0], \"|\", X1.shape[1],\"|\")\n",
    "            print(\"|\", (z+1), \"|\", df_labs[z], \"|\", model_labs[index], \"|\", var1, \"|\", var2, \"|\", var3, \"|\", var4, \"|\",var5, \"|\",var6, \"|\")\n",
    "            # Feature importance plot\n",
    "            #xgb.plot_importance(model_bp2, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance, gain', \n",
    "            #            xlabel='F score - Gain', ylabel='Features', \n",
    "            #            importance_type='gain', max_num_features=15, grid=True, show_values=False) #, values_format='{v:.2f}' )\n",
    "\n",
    "            #pyplot.savefig(r'.\\figs\\version4\\h2p\\df23\\neg_gain_m{y}_v2.png'.format(y=len(x)),  dpi=300, bbox_inches='tight')\n",
    "            #pyplot.show()\n",
    "            # create lis of feature names to be used in dependency plot so that high ranking vars are plotted\n",
    "            #features_names1 = pd.DataFrame()\n",
    "            #features_names1['columns'] = X1.columns\n",
    "            #features_names1['importances'] = model_bp2.feature_importances_\n",
    "            #features_names1.sort_values(by='importances',ascending=False,inplace=True)\n",
    "            #features_names2 = features_names1['columns'].tolist()[0:10]\n",
    "\n",
    "            #_, ax1 = plt.subplots(figsize=(9, 8), constrained_layout=True)\n",
    "\n",
    "            #display = PartialDependenceDisplay.from_estimator(model_bp2, x1_train, features_names2, ax=ax1)\n",
    "\n",
    "            #_ = display.figure_.suptitle((\"Partial dependence plots\"), fontsize=12, )\n",
    "\n",
    "            #pyplot.savefig(r'.\\figs\\version4\\h2p\\df23\\neg_partial_dep_m{y}_v2.png'.format(y=len(x)),  dpi=300, bbox_inches='tight')\n",
    "\n",
    "            #pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd7d5a9",
   "metadata": {},
   "source": [
    "**Table 2**: Model comparison for negative breaks. Subset of data records was used, excluding records that had a match with disturbance data. (Some NAs for for_age and for_con.)\n",
    "\n",
    "|ID| Data frame| Model   | MSE| RMSE| R-sq | R-sq-adj | N rows| N vars|\n",
    "| --------| --------| -------- | ------- |-------- | ------- |------- |------- |------- |\n",
    "| 1 | df0 | First variable set | 23664.83 | 153.83 | 0.386 | 0.383 | 11118 | 15 |\n",
    "| 2 | df0 | VIF variable set | 23734.41 | 154.06 | 0.384 | 0.383 | 11118 | 7 |\n",
    "| 3 | df0 | VIFplus variable set | 24011.27 | 154.96 | 0.377 | 0.375 | 11118 | 9 |\n",
    "| 4 | df6 | First variable set | 25736.86 | 160.43 | 0.652 | 1.126 | 15 | 15 |\n",
    "| 5 | df6 | VIF variable set | 32191.19 | 179.42 | 0.565 | 1.580 | 15 | 7 |\n",
    "| 6 | df6 | VIFplus variable set | 32191.19 | 179.42 | 0.565 | 1.348 | 15 | 9 |\n",
    "| 7 | df9 | First variable set | 45051.81 | 212.25 | 0.606 | 0.070 | 81 | 15 |\n",
    "| 8 | df9 | VIF variable set | 55276.32 | 235.11 | 0.517 | 0.339 | 81 | 7 |\n",
    "| 9 | df9 | VIFplus variable set | 55457.17 | 235.49 | 0.515 | 0.259 | 81 | 9 |\n",
    "| 10 | df23 | First variable set | 28267.67 | 168.13 | 0.290 | 0.276 | 2425 | 15 |\n",
    "| 11 | df23 | VIF variable set | 28986.44 | 170.25 | 0.272 | 0.265 | 2425 | 7 |\n",
    "| 12 | df23 | VIFplus variable set | 28023.82 | 167.40 | 0.296 | 0.288 | 2425 | 9 |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "020b1743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2425, 9)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check one more\n",
    "# df2.shape\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec885f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
