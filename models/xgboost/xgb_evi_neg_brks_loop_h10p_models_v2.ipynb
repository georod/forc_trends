{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9dab20",
   "metadata": {},
   "source": [
    "# Negative breaks XGBoost model - local run with loop\n",
    "\n",
    "### Peter R.\n",
    "#### 2024-03-14\n",
    "\n",
    "Note: Here I use h10p, that is Bfast breaks with h=0.1 or ~ 10%. The filtering of records with h10p is the samet than with h5p.\n",
    "\n",
    "Today (2024-03-04) I meet with MJF to discuss XGB model improvements. An important number of Bfast breaks have very wide confidence intervals (CIs) associated with the time of break. These CIs can range from about 1 month to 80+ months. These won't allow for high quality matching with yearly climate or disturbance data. For this reason, we decided to run XGB models with subsets of data, each subset has narrow CIs. We will run the following XGB models with the follwoing dataframe subsets:\n",
    "\n",
    "- Dataframe3 (df3): Records with CIs shorter than 3 16-days data points (48 days or about 1.5 months)\n",
    "\n",
    "- Dataframe6 (df6): Records with CIs shorter than 6 16-days data points (96 days or about 3 months)\n",
    "\n",
    "- Dataframe9 (df9): Records with CIs shorter than 9 16-days data points (144 days or about 5 months)\n",
    "\n",
    "- Dataframe23 (df23): Records with CIS shorter than 23 16-days data points (368 days or about 1 year)\n",
    "\n",
    "Some questions to have in mind:\n",
    "\n",
    "- How many matches with disturbance data do the above have?\n",
    "- Why does forest age become the top ranking variable with VIFplust variable set?  This variable was number 10 in other previous XGB model.\n",
    "- I am assuming that Hansen is best and that it only includes stand-replacing disturbances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "82ed8877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB version: 1.7.6\n",
      "negative breaks\n"
     ]
    }
   ],
   "source": [
    "# 2024-03-12\n",
    "# Peter R.\n",
    "# XGBoost script\n",
    "# Positive breaks, n_estimators (number of trees)=1000 and with optimal parameter from DRAC model_bp1 & early stopping\n",
    "\n",
    "#Here I am using a loop to run several models at a time\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import nan\n",
    "import xgboost as xgb\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# for feature importance plots\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#for dependency plots\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "#start = time.time()\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#print(cwd)\n",
    "\n",
    "# DRAC directory\n",
    "#os.chdir(\"/home/georod/projects/def-mfortin/georod/scripts/github/forc_trends/models/xgboost\")\n",
    "# Win directory\n",
    "os.chdir(r'C:\\Users\\Peter R\\github\\forc_trends\\models\\xgboost')\n",
    "\n",
    "\n",
    "print(\"XGB version:\", xgb.__version__)\n",
    "print(\"negative breaks\")\n",
    "\n",
    "\n",
    "# Windows\n",
    "df1 = pd.read_csv(r'.\\data\\forest_evi_breaks_negative_h10p_v3.csv', skipinitialspace=True)\n",
    "# DRAC\n",
    "#df1 = pd.read_csv(r'./data/forest_evi_breaks_positive_v2.csv', skipinitialspace=True)\n",
    "#df1.head()\n",
    "\n",
    "\n",
    "df11 = pd.get_dummies(df1, columns=['for_pro'], dtype=float)\n",
    "\n",
    "#Df0: all rows\n",
    "#df2 = df11 # N=843\n",
    "# Df3: 1.5 months, version4/df3\n",
    "#df2 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.1315068) & (df11['magnitude'] < -700)] #N= 171\n",
    "# Df6: 3 months, version4/df6\n",
    "#df2 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.2630137) & (df11['magnitude'] < -700)] #N= 450\n",
    "# Df9: 5 months, version4/df9\n",
    "# df2 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.3945205) & (df11['magnitude']< -700)] #N=523\n",
    "# Df23: 12 months, 1 year, version4/df23\n",
    "#df2 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 1.008219) & (df11['magnitude'] < -700)] #N=649\n",
    "\n",
    "# Df23v2: 12 months, 1 year, bounded by same year, version4/df23v2\n",
    "#df2 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 1.008219) & (df11['magnitude'] < -700)] # N=2291\n",
    "#df2 = df2.loc[(np.floor(df2['brkdate95'])==np.floor(df2['brkdate25']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a599c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Df0: all rows\n",
    "df0 = df11 # N=19204\n",
    "# Df3: 1.5 months, version4/df3\n",
    "df3 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.09) & (df11['magnitude'] < -500)] #N= 4, 0.1315068\n",
    "#Df6: 3 months, version4/df6\n",
    "df6 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.2630137) & (df11['magnitude'] < -500)] #N= 205\n",
    "# Df9: 5 months, version4/df9\n",
    "df9 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.3945205) & (df11['magnitude']< -500)] #N=487\n",
    "# Df23: 12 months, 1 year, version4/df23\n",
    "df23 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 1.008219) & (df11['magnitude'] < -500)] #N=5347\n",
    "\n",
    "dfall = [df0,df3, df6, df9, df23]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9078a73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                pix    year  brk  brkdate25   brkdate  brkdate95    magnitude  \\\n",
      "count      4.000000     4.0  4.0      4.000     4.000      4.000     4.000000   \n",
      "mean   10857.000000  2018.0  0.0   2018.478  2018.522   2018.565 -2677.033000   \n",
      "std      984.583499     0.0  0.0      0.000     0.000      0.000   243.904668   \n",
      "min    10085.000000  2018.0  0.0   2018.478  2018.522   2018.565 -2990.034000   \n",
      "25%    10085.750000  2018.0  0.0   2018.478  2018.522   2018.565 -2774.805750   \n",
      "50%    10600.500000  2018.0  0.0   2018.478  2018.522   2018.565 -2658.146500   \n",
      "75%    11371.750000  2018.0  0.0   2018.478  2018.522   2018.565 -2560.373750   \n",
      "max    12142.000000  2018.0  0.0   2018.478  2018.522   2018.565 -2401.805000   \n",
      "\n",
      "       no_brk  fire_year  harv_year  ...   bffp  bffp_lag1  bffp_lag2  \\\n",
      "count     0.0        4.0        0.0  ...    4.0    4.00000        4.0   \n",
      "mean      NaN     2018.0        NaN  ...  149.0  141.90125      146.0   \n",
      "std       NaN        0.0        NaN  ...    0.0    0.19750        0.0   \n",
      "min       NaN     2018.0        NaN  ...  149.0  141.60500      146.0   \n",
      "25%       NaN     2018.0        NaN  ...  149.0  141.90125      146.0   \n",
      "50%       NaN     2018.0        NaN  ...  149.0  142.00000      146.0   \n",
      "75%       NaN     2018.0        NaN  ...  149.0  142.00000      146.0   \n",
      "max       NaN     2018.0        NaN  ...  149.0  142.00000      146.0   \n",
      "\n",
      "        bffp_lag3   effp  effp_lag1  effp_lag2   effp_lag3  for_pro_0  \\\n",
      "count    4.000000    4.0        4.0        4.0    4.000000        4.0   \n",
      "mean   147.918250  267.0      275.0      271.0  268.306000        1.0   \n",
      "std      0.105427    0.0        0.0        0.0    0.295677        0.0   \n",
      "min    147.779000  267.0      275.0      271.0  268.000000        1.0   \n",
      "25%    147.865250  267.0      275.0      271.0  268.102000        1.0   \n",
      "50%    147.947000  267.0      275.0      271.0  268.282500        1.0   \n",
      "75%    148.000000  267.0      275.0      271.0  268.486500        1.0   \n",
      "max    148.000000  267.0      275.0      271.0  268.659000        1.0   \n",
      "\n",
      "       for_pro_1  \n",
      "count        4.0  \n",
      "mean         0.0  \n",
      "std          0.0  \n",
      "min          0.0  \n",
      "25%          0.0  \n",
      "50%          0.0  \n",
      "75%          0.0  \n",
      "max          0.0  \n",
      "\n",
      "[8 rows x 158 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dfall[1].describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "45deb38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix</th>\n",
       "      <th>year</th>\n",
       "      <th>brk</th>\n",
       "      <th>brkdate25</th>\n",
       "      <th>brkdate</th>\n",
       "      <th>brkdate95</th>\n",
       "      <th>magnitude</th>\n",
       "      <th>no_brk</th>\n",
       "      <th>fire_year</th>\n",
       "      <th>harv_year</th>\n",
       "      <th>...</th>\n",
       "      <th>bffp</th>\n",
       "      <th>bffp_lag1</th>\n",
       "      <th>bffp_lag2</th>\n",
       "      <th>bffp_lag3</th>\n",
       "      <th>effp</th>\n",
       "      <th>effp_lag1</th>\n",
       "      <th>effp_lag2</th>\n",
       "      <th>effp_lag3</th>\n",
       "      <th>for_pro_0</th>\n",
       "      <th>for_pro_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15543</th>\n",
       "      <td>10085</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>2018.478</td>\n",
       "      <td>2018.522</td>\n",
       "      <td>2018.565</td>\n",
       "      <td>-2990.034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>149.0</td>\n",
       "      <td>142.000</td>\n",
       "      <td>146.0</td>\n",
       "      <td>148.000</td>\n",
       "      <td>267.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>268.136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15544</th>\n",
       "      <td>10086</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>2018.478</td>\n",
       "      <td>2018.522</td>\n",
       "      <td>2018.565</td>\n",
       "      <td>-2703.063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>149.0</td>\n",
       "      <td>141.605</td>\n",
       "      <td>146.0</td>\n",
       "      <td>147.894</td>\n",
       "      <td>267.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>268.659</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15979</th>\n",
       "      <td>12142</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>2018.478</td>\n",
       "      <td>2018.522</td>\n",
       "      <td>2018.565</td>\n",
       "      <td>-2401.805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>149.0</td>\n",
       "      <td>142.000</td>\n",
       "      <td>146.0</td>\n",
       "      <td>148.000</td>\n",
       "      <td>267.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>268.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19013</th>\n",
       "      <td>11115</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>2018.478</td>\n",
       "      <td>2018.522</td>\n",
       "      <td>2018.565</td>\n",
       "      <td>-2613.230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>149.0</td>\n",
       "      <td>142.000</td>\n",
       "      <td>146.0</td>\n",
       "      <td>147.779</td>\n",
       "      <td>267.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>268.429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pix  year  brk  brkdate25   brkdate  brkdate95  magnitude  no_brk  \\\n",
       "15543  10085  2018    0   2018.478  2018.522   2018.565  -2990.034     NaN   \n",
       "15544  10086  2018    0   2018.478  2018.522   2018.565  -2703.063     NaN   \n",
       "15979  12142  2018    0   2018.478  2018.522   2018.565  -2401.805     NaN   \n",
       "19013  11115  2018    0   2018.478  2018.522   2018.565  -2613.230     NaN   \n",
       "\n",
       "       fire_year  harv_year  ...   bffp  bffp_lag1  bffp_lag2  bffp_lag3  \\\n",
       "15543     2018.0        NaN  ...  149.0    142.000      146.0    148.000   \n",
       "15544     2018.0        NaN  ...  149.0    141.605      146.0    147.894   \n",
       "15979     2018.0        NaN  ...  149.0    142.000      146.0    148.000   \n",
       "19013     2018.0        NaN  ...  149.0    142.000      146.0    147.779   \n",
       "\n",
       "        effp  effp_lag1  effp_lag2  effp_lag3  for_pro_0  for_pro_1  \n",
       "15543  267.0      275.0      271.0    268.136        1.0        0.0  \n",
       "15544  267.0      275.0      271.0    268.659        1.0        0.0  \n",
       "15979  267.0      275.0      271.0    268.000        1.0        0.0  \n",
       "19013  267.0      275.0      271.0    268.429        1.0        0.0  \n",
       "\n",
       "[4 rows x 158 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dfall[1]).head()\n",
    "#range(len(dfall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "57147b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1 | df0 | First variable set | 31340.33 | 177.03 | 0.439 | 0.438 | 19204 | 15 |\n",
      "| 1 | df0 | VIF variable set | 33823.29 | 183.91 | 0.395 | 0.394 | 19204 | 7 |\n",
      "| 1 | df0 | VIFplus variable set | 31826.08 | 178.40 | 0.431 | 0.430 | 19204 | 9 |\n",
      "| 2 | df3 | First variable set | 100831.55 | 317.54 | -3.444 | 1.317 | 4 | 15 |\n",
      "| 2 | df3 | VIF variable set | 100831.55 | 317.54 | -3.444 | 1.741 | 4 | 7 |\n",
      "| 2 | df3 | VIFplus variable set | 100831.55 | 317.54 | -3.444 | 1.556 | 4 | 9 |\n",
      "| 3 | df6 | First variable set | 89389.22 | 298.98 | 0.697 | 0.610 | 205 | 15 |\n",
      "| 3 | df6 | VIF variable set | 125055.03 | 353.63 | 0.576 | 0.527 | 205 | 7 |\n",
      "| 3 | df6 | VIFplus variable set | 86303.03 | 293.77 | 0.707 | 0.662 | 205 | 9 |\n",
      "| 4 | df9 | First variable set | 76544.90 | 276.67 | 0.570 | 0.525 | 487 | 15 |\n",
      "| 4 | df9 | VIF variable set | 83031.21 | 288.15 | 0.533 | 0.512 | 487 | 7 |\n",
      "| 4 | df9 | VIFplus variable set | 70910.51 | 266.29 | 0.601 | 0.578 | 487 | 9 |\n",
      "| 5 | df23 | First variable set | 35557.17 | 188.57 | 0.439 | 0.434 | 5347 | 15 |\n",
      "| 5 | df23 | VIF variable set | 42273.44 | 205.61 | 0.333 | 0.330 | 5347 | 7 |\n",
      "| 5 | df23 | VIFplus variable set | 35384.48 | 188.11 | 0.441 | 0.439 | 5347 | 9 |\n"
     ]
    }
   ],
   "source": [
    "# loop version\n",
    "\n",
    "cols1 = ['for_age', 'for_con', 'map', 'map_lag1', 'map_lag2', 'map_lag3', 'mat', 'mat_lag1', 'mat_lag2', 'mat_lag3', 'rh', 'rh_lag1', 'rh_lag2', 'rh_lag3', 'for_pro_0']\n",
    "cols2 = ['for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt_lag1', 'dd5_wt_lag3']\n",
    "cols3 = ['for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt_lag1', 'dd5_wt_lag3', 'for_age', 'for_pro_0']\n",
    "\n",
    "df_labs = ['df0','df3', 'df6', 'df9', 'df23']\n",
    "model_labs = ['First variable set', 'VIF variable set', 'VIFplus variable set']\n",
    "\n",
    "for z in range(len(dfall)):\n",
    "    list_of_vars = [[cols1], [cols2], [cols3]]\n",
    "    for index, list in enumerate(list_of_vars):\n",
    "        for x in list:\n",
    "            #print(x)\n",
    "            X1 = dfall[z][x]\n",
    "            #print(X1.describe())\n",
    "            y1 = dfall[z].iloc[:,6].abs()\n",
    "            seed = 7 # random seed to help with replication\n",
    "            testsize1 = 0.33 # percent of records to test after training\n",
    "            x1_train, x1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=testsize1, random_state=seed) # Split data set. Note the 'stratify' option\n",
    "            model_bp2 = XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
    "                 colsample_bylevel=None, colsample_bynode=None,\n",
    "                 colsample_bytree=None, early_stopping_rounds=50,\n",
    "                 enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "                 gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "                 interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
    "                 max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "                 max_delta_step=None, max_depth=8, max_leaves=None,\n",
    "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "                 n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
    "                 predictor=None, random_state=42, reg_lambda=10, reg_alpha=1)\n",
    "               # EVALUATION (with test)\n",
    "            eval_set = [(x1_train, y1_train), (x1_test, y1_test)]\n",
    "                #UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
    "            model_bp2.fit(x1_train, y1_train, eval_set=eval_set, verbose=False)\n",
    "                # make predictions for test data\n",
    "            y_pred = model_bp2.predict(x1_test)\n",
    "            predictions = [round(value) for value in y_pred]\n",
    "                # retrieve performance metrics\n",
    "            results = model_bp2.evals_result()\n",
    "            mse = mean_squared_error(y1_test, y_pred)\n",
    "                #r2 = explained_variance_score(y1_test, ypred)\n",
    "            r2 = r2_score(y1_test, y_pred)\n",
    "                # adjusted R-squared\n",
    "            adj_r2 = 1 - (((1-r2) * (len(y1_test)-1))/(len(y1_test)-x1_test.shape[1]-1))\n",
    "\n",
    "            #print(\"MSE: %.2f\" % mse)\n",
    "            var1 = \"%.2f\" % mse\n",
    "\n",
    "            #print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
    "            var2 = \"%.2f\" % (mse**(1/2.0))\n",
    "\n",
    "            #print(\"R-sq: %.3f\" % r2)\n",
    "            var3 = \"%.3f\" % r2\n",
    "\n",
    "            #print(\"R-sq-adj: %.3f\" % adj_r2)\n",
    "            var4 = \"%.3f\" % adj_r2\n",
    "\n",
    "            var5 = X1.shape[0]\n",
    "            var6 = X1.shape[1]\n",
    "\n",
    "            # row for table\n",
    "            #print(\"| %.2f\" % mse, \"| %.2f\" % (mse**(1/2.0)), \"| %.3f\" % r2, \"| %.3f\" % adj_r2, \"|\", X1.shape[0], \"|\", X1.shape[1],\"|\")\n",
    "            #print(\"|\", z+1, \"|\", df_labs[z], \"|\", model_labs[index], \"| %.2f\" % mse, \"| %.2f\" % (mse**(1/2.0)), \"| %.3f\" % r2, \"| %.3f\" % adj_r2, \"|\", X1.shape[0], \"|\", X1.shape[1],\"|\")\n",
    "            print(\"|\", (z+1), \"|\", df_labs[z], \"|\", model_labs[index], \"|\", var1, \"|\", var2, \"|\", var3, \"|\", var4, \"|\",var5, \"|\",var6, \"|\")\n",
    "            # Feature importance plot\n",
    "            #xgb.plot_importance(model_bp2, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance, gain', \n",
    "            #            xlabel='F score - Gain', ylabel='Features', \n",
    "            #            importance_type='gain', max_num_features=15, grid=True, show_values=False) #, values_format='{v:.2f}' )\n",
    "\n",
    "            #pyplot.savefig(r'.\\figs\\version4\\h2p\\df23\\neg_gain_m{y}_v1.png'.format(y=len(x)),  dpi=300, bbox_inches='tight')\n",
    "            #pyplot.show()\n",
    "            # create lis of feature names to be used in dependency plot so that high ranking vars are plotted\n",
    "            #features_names1 = pd.DataFrame()\n",
    "           # features_names1['columns'] = X1.columns\n",
    "           # features_names1['importances'] = model_bp2.feature_importances_\n",
    "           # features_names1.sort_values(by='importances',ascending=False,inplace=True)\n",
    "           # features_names2 = features_names1['columns'].tolist()[0:10]\n",
    "\n",
    "           # _, ax1 = plt.subplots(figsize=(9, 8), constrained_layout=True)\n",
    "\n",
    "           # display = PartialDependenceDisplay.from_estimator(model_bp2, x1_train, features_names2, ax=ax1)\n",
    "\n",
    "           # _ = display.figure_.suptitle((\"Partial dependence plots\"), fontsize=12, )\n",
    "\n",
    "           # pyplot.savefig(r'.\\figs\\version4\\h2p\\df23\\neg_partial_dep_m{y}_v1.png'.format(y=len(x)),  dpi=300, bbox_inches='tight')\n",
    "\n",
    "           # pyplot.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1ddf3",
   "metadata": {},
   "source": [
    "**Table 1**: Model comparison for negative breaks. Standard data set with all records (including NAs for for_age and for_con).\n",
    "\n",
    "\n",
    "|ID|Data frame| Model   | MSE| RMSE| R-sq | R-sq-adj |N rows| N vars|\n",
    "| --------| --------| --------| --------| -------- | ------- |-------- | ------- |------- |\n",
    "| 1 | df0 | First variable set | 31340.33 | 177.03 | 0.439 | 0.438 | 19204 | 15 |\n",
    "| 2 | df0 | VIF variable set | 33823.29 | 183.91 | 0.395 | 0.394 | 19204 | 7 |\n",
    "| 3 | df0 | VIFplus variable set | 31826.08 | 178.40 | 0.431 | 0.430 | 19204 | 9 |\n",
    "| 4 | df3 | First variable set | 100831.55 | 317.54 | -3.444 | 1.317 | 4 | 15 |\n",
    "| 5 | df3 | VIF variable set | 100831.55 | 317.54 | -3.444 | 1.741 | 4 | 7 |\n",
    "| 6 | df3 | VIFplus variable set | 100831.55 | 317.54 | -3.444 | 1.556 | 4 | 9 |\n",
    "| 7 | df6 | First variable set | 89389.22 | 298.98 | 0.697 | 0.610 | 205 | 15 |\n",
    "| 8 | df6 | VIF variable set | 125055.03 | 353.63 | 0.576 | 0.527 | 205 | 7 |\n",
    "| 9 | df6 | VIFplus variable set | 86303.03 | 293.77 | 0.707 | 0.662 | 205 | 9 |\n",
    "| 10 | df9 | First variable set | 76544.90 | 276.67 | 0.570 | 0.525 | 487 | 15 |\n",
    "| 11 | df9 | VIF variable set | 83031.21 | 288.15 | 0.533 | 0.512 | 487 | 7 |\n",
    "| 12 | df9 | VIFplus variable set | 70910.51 | 266.29 | 0.601 | 0.578 | 487 | 9 |\n",
    "| 13 | df23 | First variable set | 35557.17 | 188.57 | 0.439 | 0.434 | 5347 | 15 |\n",
    "| 14 | df23 | VIF variable set | 42273.44 | 205.61 | 0.333 | 0.330 | 5347 | 7 |\n",
    "| 15 | df23 | VIFplus variable set | 35384.48 | 188.11 | 0.441 | 0.439 | 5347 | 9 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "35a6f9d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Misc cmds\n",
    "#Describe the data\n",
    "#X1 = df2[x]\n",
    "#print(X1.shape)\n",
    "#x\n",
    "#print(len(cols1))\n",
    "# Count NAs per columns to check that step above worked #mat 607 before, now 0\n",
    "#X1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e3e104b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b80b06",
   "metadata": {},
   "source": [
    "### Models without records that have disturbance matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ffc814",
   "metadata": {},
   "source": [
    "When dealing with positive forest EVI breaks, I can't remove records matched to Hansen et al.'s disturbance data as there are no such matched records. This makes sense as positive breaks should not be matched to disturbances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "31116268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many records are matched to disturbance data?\n",
    "#print(df2[['hansen_year']].describe()) # N=2775\n",
    "#print(df2[['magnitude', 'fire_year', 'harv_year', 'canlad_year', 'hansen_year']].describe()) # Hansen=? with df4; Hansen=648 with df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "28c2999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Df0: all records\n",
    "#df0 = df11 # N=687\n",
    "df0 = df0.loc[df0['hansen_year'].isnull()]\n",
    "# Df3: 1.5 months\n",
    "#df3 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.1315068) & (df11['magnitude']> 700)] #N=40\n",
    "df3 = df3.loc[df3['hansen_year'].isnull()]\n",
    "# Df6: 3 months\n",
    "#df6 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.2630137) & (df11['magnitude']> 700)] #N=168\n",
    "df6 = df6.loc[df6['hansen_year'].isnull()] #N=168\n",
    "# Df9: 5 months, version 4\n",
    "#df9 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 0.3945205) & (df11['magnitude']> 700)] #N=649\n",
    "df9 = df9.loc[df9['hansen_year'].isnull()] \n",
    "# Df23: 12 months, 1 year, version 5\n",
    "#df23 = df11.loc[(df11['brkdate95']-df11['brkdate25'] <= 1.008219) & (df11['magnitude']> 700)] #N=2216\n",
    "df23 = df23.loc[df23['hansen_year'].isnull()] \n",
    "#dfall = [[df0],[df3], [df6], [df9], [df23]]\n",
    "#dfall = [df0, df3, df6, df9, df23]\n",
    "dfall = [df0, df6, df9, df23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dd6ee948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df2[['canlad_year']].describe()) # 2483; 247\n",
    "#print(df2[['harv_year']].describe()) # 1187; 204\n",
    "#print(df2[['fire_year']].describe()) # 139; 107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bfcfc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This produces an empty df as there are no records\n",
    "#df3 = df2.drop(df2[df2.hansen_year > 0].index)\n",
    "#X3.tail\n",
    "#X3.shape\n",
    "#df3.describe()\n",
    "#df2.drop(df2[df2.hansen_year > 0].index, inplace=True) # gives a warning\n",
    "#df2.shape\n",
    "\n",
    "#df2 = df2.loc[df2['hansen_year'] > 0] # 2775\n",
    "\n",
    "#df2 = df2.loc[df2['hansen_year'].isnull()]\n",
    "\n",
    "#df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ea2bf42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.drop(df2[df2.hansen_year > 0].index).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4baf9489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 158)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfall[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "749bc1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1 | df0 | First variable set | 23664.83 | 153.83 | 0.386 | 0.383 | 11118 | 15 |\n",
      "| 1 | df0 | VIF variable set | 23734.41 | 154.06 | 0.384 | 0.383 | 11118 | 7 |\n",
      "| 1 | df0 | VIFplus variable set | 24011.27 | 154.96 | 0.377 | 0.375 | 11118 | 9 |\n",
      "| 2 | df6 | First variable set | 25736.86 | 160.43 | 0.652 | 1.126 | 15 | 15 |\n",
      "| 2 | df6 | VIF variable set | 32191.19 | 179.42 | 0.565 | 1.580 | 15 | 7 |\n",
      "| 2 | df6 | VIFplus variable set | 32191.19 | 179.42 | 0.565 | 1.348 | 15 | 9 |\n",
      "| 3 | df9 | First variable set | 45051.81 | 212.25 | 0.606 | 0.070 | 81 | 15 |\n",
      "| 3 | df9 | VIF variable set | 55276.32 | 235.11 | 0.517 | 0.339 | 81 | 7 |\n",
      "| 3 | df9 | VIFplus variable set | 55457.17 | 235.49 | 0.515 | 0.259 | 81 | 9 |\n",
      "| 4 | df23 | First variable set | 28267.67 | 168.13 | 0.290 | 0.276 | 2425 | 15 |\n",
      "| 4 | df23 | VIF variable set | 28986.44 | 170.25 | 0.272 | 0.265 | 2425 | 7 |\n",
      "| 4 | df23 | VIFplus variable set | 28023.82 | 167.40 | 0.296 | 0.288 | 2425 | 9 |\n"
     ]
    }
   ],
   "source": [
    "# loop version\n",
    "\n",
    "cols1 = ['for_age', 'for_con', 'map', 'map_lag1', 'map_lag2', 'map_lag3', 'mat', 'mat_lag1', 'mat_lag2', 'mat_lag3', 'rh', 'rh_lag1', 'rh_lag2', 'rh_lag3', 'for_pro_0']\n",
    "cols2 = ['for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt_lag1', 'dd5_wt_lag3']\n",
    "cols3 = ['for_con', 'cmi_sm', 'cmi_sm_lag1', 'cmi_sm_lag2', 'cmi_sm_lag3', 'dd5_wt_lag1', 'dd5_wt_lag3', 'for_age', 'for_pro_0']\n",
    "\n",
    "#df_labs = ['df0','df3', 'df6', 'df9', 'df23']\n",
    "df_labs = ['df0', 'df6','df9', 'df23']\n",
    "model_labs = ['First variable set', 'VIF variable set', 'VIFplus variable set']\n",
    "\n",
    "for z in range(len(dfall)):\n",
    "    list_of_vars = [ [cols1], [cols2], [cols3]]\n",
    "    for index, list in enumerate(list_of_vars):\n",
    "        for x in list:\n",
    "            #print(x)\n",
    "            X1 = dfall[z][x]\n",
    "            #print(X1.describe())\n",
    "            y1 = dfall[z].iloc[:,6].abs()\n",
    "            seed = 7 # random seed to help with replication\n",
    "            testsize1 = 0.33 # percent of records to test after training\n",
    "            x1_train, x1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=testsize1, random_state=seed) # Split data set. Note the 'stratify' option\n",
    "            model_bp2 = XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
    "                 colsample_bylevel=None, colsample_bynode=None,\n",
    "                 colsample_bytree=None, early_stopping_rounds=50,\n",
    "                 enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "                 gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "                 interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
    "                 max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "                 max_delta_step=None, max_depth=8, max_leaves=None,\n",
    "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "                 n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
    "                 predictor=None, random_state=42, reg_lambda=10, reg_alpha=1)\n",
    "               # EVALUATION (with test)\n",
    "            eval_set = [(x1_train, y1_train), (x1_test, y1_test)]\n",
    "                #UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
    "            model_bp2.fit(x1_train, y1_train, eval_set=eval_set, verbose=False)\n",
    "                # make predictions for test data\n",
    "            y_pred = model_bp2.predict(x1_test)\n",
    "            predictions = [round(value) for value in y_pred]\n",
    "                # retrieve performance metrics\n",
    "            results = model_bp2.evals_result()\n",
    "            mse = mean_squared_error(y1_test, y_pred)\n",
    "                #r2 = explained_variance_score(y1_test, ypred)\n",
    "            r2 = r2_score(y1_test, y_pred)\n",
    "                # adjusted R-squared\n",
    "            adj_r2 = 1 - (((1-r2) * (len(y1_test)-1))/(len(y1_test)-x1_test.shape[1]-1))\n",
    "\n",
    "            #print(\"MSE: %.2f\" % mse)\n",
    "            var1 = \"%.2f\" % mse\n",
    "\n",
    "            #print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
    "            var2 = \"%.2f\" % (mse**(1/2.0))\n",
    "\n",
    "            #print(\"R-sq: %.3f\" % r2)\n",
    "            var3 = \"%.3f\" % r2\n",
    "\n",
    "            #print(\"R-sq-adj: %.3f\" % adj_r2)\n",
    "            var4 = \"%.3f\" % adj_r2\n",
    "\n",
    "            var5 = X1.shape[0]\n",
    "            var6 = X1.shape[1]\n",
    "\n",
    "             # row for table\n",
    "            #print(\"| %.2f\" % mse, \"| %.2f\" % (mse**(1/2.0)), \"| %.3f\" % r2, \"| %.3f\" % adj_r2, \"|\", X1.shape[0], \"|\", X1.shape[1],\"|\")\n",
    "            print(\"|\", (z+1), \"|\", df_labs[z], \"|\", model_labs[index], \"|\", var1, \"|\", var2, \"|\", var3, \"|\", var4, \"|\",var5, \"|\",var6, \"|\")\n",
    "            # Feature importance plot\n",
    "            #xgb.plot_importance(model_bp2, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance, gain', \n",
    "            #            xlabel='F score - Gain', ylabel='Features', \n",
    "            #            importance_type='gain', max_num_features=15, grid=True, show_values=False) #, values_format='{v:.2f}' )\n",
    "\n",
    "            #pyplot.savefig(r'.\\figs\\version4\\h2p\\df23\\neg_gain_m{y}_v2.png'.format(y=len(x)),  dpi=300, bbox_inches='tight')\n",
    "            #pyplot.show()\n",
    "            # create lis of feature names to be used in dependency plot so that high ranking vars are plotted\n",
    "            #features_names1 = pd.DataFrame()\n",
    "            #features_names1['columns'] = X1.columns\n",
    "            #features_names1['importances'] = model_bp2.feature_importances_\n",
    "            #features_names1.sort_values(by='importances',ascending=False,inplace=True)\n",
    "            #features_names2 = features_names1['columns'].tolist()[0:10]\n",
    "\n",
    "            #_, ax1 = plt.subplots(figsize=(9, 8), constrained_layout=True)\n",
    "\n",
    "            #display = PartialDependenceDisplay.from_estimator(model_bp2, x1_train, features_names2, ax=ax1)\n",
    "\n",
    "            #_ = display.figure_.suptitle((\"Partial dependence plots\"), fontsize=12, )\n",
    "\n",
    "            #pyplot.savefig(r'.\\figs\\version4\\h2p\\df23\\neg_partial_dep_m{y}_v2.png'.format(y=len(x)),  dpi=300, bbox_inches='tight')\n",
    "\n",
    "            #pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd7d5a9",
   "metadata": {},
   "source": [
    "**Table 2**: Model comparison for negative breaks. Subset of data records was used, excluding records that had a match with disturbance data. (Some NAs for for_age and for_con.)\n",
    "\n",
    "|ID| Data frame| Model   | MSE| RMSE| R-sq | R-sq-adj | N rows| N vars|\n",
    "| --------| --------| -------- | ------- |-------- | ------- |------- |------- |------- |\n",
    "| 1 | df0 | First variable set | 23664.83 | 153.83 | 0.386 | 0.383 | 11118 | 15 |\n",
    "| 2 | df0 | VIF variable set | 23734.41 | 154.06 | 0.384 | 0.383 | 11118 | 7 |\n",
    "| 3 | df0 | VIFplus variable set | 24011.27 | 154.96 | 0.377 | 0.375 | 11118 | 9 |\n",
    "| 4 | df6 | First variable set | 25736.86 | 160.43 | 0.652 | 1.126 | 15 | 15 |\n",
    "| 5 | df6 | VIF variable set | 32191.19 | 179.42 | 0.565 | 1.580 | 15 | 7 |\n",
    "| 6 | df6 | VIFplus variable set | 32191.19 | 179.42 | 0.565 | 1.348 | 15 | 9 |\n",
    "| 7 | df9 | First variable set | 45051.81 | 212.25 | 0.606 | 0.070 | 81 | 15 |\n",
    "| 8 | df9 | VIF variable set | 55276.32 | 235.11 | 0.517 | 0.339 | 81 | 7 |\n",
    "| 9 | df9 | VIFplus variable set | 55457.17 | 235.49 | 0.515 | 0.259 | 81 | 9 |\n",
    "| 10 | df23 | First variable set | 28267.67 | 168.13 | 0.290 | 0.276 | 2425 | 15 |\n",
    "| 11 | df23 | VIF variable set | 28986.44 | 170.25 | 0.272 | 0.265 | 2425 | 7 |\n",
    "| 12 | df23 | VIFplus variable set | 28023.82 | 167.40 | 0.296 | 0.288 | 2425 | 9 |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "020b1743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2425, 9)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check one more\n",
    "# df2.shape\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec885f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
